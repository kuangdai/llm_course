{"id":"b7c4f7d5-4b95-410b-8e97-e293087ce7f0","data":{"nodes":[{"data":{"description":"Get chat inputs from the Playground.","display_name":"Chat Input","id":"ChatInput-228Fo","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Get chat inputs from the Playground.","display_name":"Chat Input","documentation":"","edited":false,"field_order":["input_value","store_message","sender","sender_name","session_id","files"],"frozen":false,"icon":"ChatInput","output_types":[],"outputs":[{"cache":true,"display_name":"Message","method":"message_response","name":"message","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n"},"files":{"advanced":true,"display_name":"Files","dynamic":false,"fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"file_path":"","info":"Files to be sent with the message.","list":true,"name":"files","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"file","value":""},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Message to be passed as input.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"I composed one. Any poem similar to it?\n\nIn whispers soft, the autumn leaves,\nDance beneath the quiet eaves.\nGolden hues in cool breeze sway,\nAs twilight steals the warmth of day."},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Type of sender.","name":"sender","options":["Machine","User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"User"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Name of the sender.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"User"},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"should_store_message":{"_input_type":"BoolInput","advanced":true,"display_name":"Store Messages","dynamic":false,"info":"Store the message in the history.","list":false,"name":"should_store_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}},"lf_version":"1.0.18"},"type":"ChatInput"},"dragging":false,"height":289,"id":"ChatInput-228Fo","position":{"x":428.5822388583747,"y":617.2410592842022},"positionAbsolute":{"x":428.5822388583747,"y":617.2410592842022},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Retrieves stored chat messages from Langflow tables or an external memory.","display_name":"Chat Memory","id":"Memory-5frTm","node":{"base_classes":["BaseChatMemory","Data","Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Retrieves stored chat messages from Langflow tables or an external memory.","display_name":"Chat Memory","documentation":"","edited":false,"field_order":["memory","sender","sender_name","n_messages","session_id","order","template"],"frozen":false,"icon":"message-square-more","output_types":[],"outputs":[{"cache":true,"display_name":"Messages (Data)","method":"retrieve_messages","name":"messages","selected":"Data","types":["Data"],"value":"__UNDEFINED__"},{"cache":true,"display_name":"Messages (Text)","method":"retrieve_messages_as_text","name":"messages_text","selected":"Message","types":["Message"],"value":"__UNDEFINED__"},{"cache":true,"display_name":"Memory","method":"build_lc_memory","name":"lc_memory","selected":"BaseChatMemory","types":["BaseChatMemory"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langchain.memory import ConversationBufferMemory\n\nfrom langflow.custom import Component\nfrom langflow.field_typing import BaseChatMemory\nfrom langflow.helpers.data import data_to_text\nfrom langflow.inputs import HandleInput\nfrom langflow.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import LCBuiltinChatMemory, get_messages\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER\n\n\nclass MemoryComponent(Component):\n    display_name = \"Chat Memory\"\n    description = \"Retrieves stored chat messages from Langflow tables or an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"BaseChatMessageHistory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the Langflow tables.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, \"Machine and User\"],\n            value=\"Machine and User\",\n            info=\"Filter by sender type.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Filter by sender name.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Messages (Data)\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Messages (Text)\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n        Output(display_name=\"Memory\", name=\"lc_memory\", method=\"build_lc_memory\"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == \"Machine and User\":\n            sender = None\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = self.memory.messages\n            # langchain memories are supposed to return messages in ascending order\n            if order == \"DESC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n            if sender:\n                expected_type = MESSAGE_SENDER_AI if sender == MESSAGE_SENDER_AI else MESSAGE_SENDER_USER\n                stored = [m for m in stored if m.type == expected_type]\n        else:\n            stored = get_messages(\n                sender=sender,\n                sender_name=sender_name,\n                session_id=session_id,\n                limit=n_messages,\n                order=order,\n            )\n        self.status = stored\n        return stored\n\n    def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    def build_lc_memory(self) -> BaseChatMemory:\n        if self.memory:\n            chat_memory = self.memory\n        else:\n            chat_memory = LCBuiltinChatMemory(flow_id=self.flow_id, session_id=self.session_id)\n        return ConversationBufferMemory(chat_memory=chat_memory)\n"},"memory":{"advanced":false,"display_name":"External Memory","dynamic":false,"info":"Retrieve messages from an external memory. If empty, it will use the Langflow tables.","input_types":["BaseChatMessageHistory"],"list":false,"name":"memory","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"n_messages":{"advanced":true,"display_name":"Number of Messages","dynamic":false,"info":"Number of messages to retrieve.","list":false,"name":"n_messages","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":100},"order":{"advanced":true,"display_name":"Order","dynamic":false,"info":"Order of the messages.","name":"order","options":["Ascending","Descending"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"Ascending"},"sender":{"advanced":true,"display_name":"Sender Type","dynamic":false,"info":"Filter by sender type.","name":"sender","options":["Machine","User","Machine and User"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"Machine and User"},"sender_name":{"advanced":true,"display_name":"Sender Name","dynamic":false,"info":"Filter by sender name.","input_types":["Message"],"list":false,"load_from_db":false,"name":"sender_name","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"session_id":{"advanced":true,"display_name":"Session ID","dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","input_types":["Message"],"list":false,"load_from_db":false,"name":"session_id","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"template":{"advanced":true,"display_name":"Template","dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"{sender_name}: {text}"}},"lf_version":"1.0.18"},"type":"Memory"},"dragging":false,"height":347,"id":"Memory-5frTm","position":{"x":4120.699123033176,"y":1346.5898659986117},"positionAbsolute":{"x":4120.699123033176,"y":1346.5898659986117},"selected":false,"type":"genericNode","width":384},{"id":"MyLLMModel-LbF5I","type":"genericNode","position":{"x":1611.5441802765818,"y":-808.7408792282913},"data":{"type":"MyLLMModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List, Optional, Any\n\nimport requests\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain.schema import LLMResult, Generation\nfrom langchain_core.language_models import BaseLLM\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    StrInput,\n    FloatInput,\n    IntInput,\n)\n\nimport re\n\nclass CustomLLMComponent(LCModelComponent):\n    display_name = \"Generative LLM\"\n    description = \"Generates text using a custom LLM server.\"\n    icon = \"chat\"\n    name = \"MyLLMModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"server_url\",\n            display_name=\"Server URL\",\n            info=\"URL for the custom server.\",\n            value=\"https://ABC.loca.lt\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.7,\n            info=\"Sampling temperature for text generation.\",\n            advanced=False,\n        ),\n        IntInput(\n            name=\"max_new_tokens\",\n            display_name=\"Max New Tokens\",\n            value=50,\n            info=\"Maximum number of tokens to generate.\",\n            advanced=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:\n        # Instantiate CustomLLM with the appropriate parameters\n        return self.CustomLLM(\n            server_url=self.server_url + \"/generate\",\n            temperature=self.temperature,\n            max_new_tokens=self.max_new_tokens\n        )\n\n    class CustomLLM(BaseLLM):\n        \"\"\"Wrapper class for custom LLM model to comply with the LanguageModel interface.\"\"\"\n\n        # Define fields as class-level variables\n        server_url: str\n        temperature: float\n        max_new_tokens: int\n\n        def _call(\n                self,\n                prompt: str,\n                stop: Optional[List[str]] = None,  # noqa\n                run_manager: Optional[CallbackManagerForLLMRun] = None,  # noqa\n                **kwargs: Any,  # noqa\n        ) -> str:\n            \"\"\"Generate text from the custom LLM model.\"\"\"\n            payload = {\n                \"text\": prompt,\n                \"temperature\": self.temperature,\n                \"max_new_tokens\": self.max_new_tokens\n            }\n            headers = {\n                \"Content-Type\": \"application/json\"\n            }\n            try:\n                response = requests.post(self.server_url, json=payload, headers=headers)\n                response.raise_for_status()\n                result = response.json()\n                return result.get(\"generated_text\", \"No generated text returned.\")\n            except requests.RequestException as e:\n                return f\"Error generating text: {e}\"\n\n        def _generate(\n                self,\n                prompts: List[str],\n                stop: Optional[List[str]] = None,\n                run_manager: Optional[CallbackManagerForLLMRun] = None,\n                **kwargs: Any,\n        ) -> LLMResult:\n            \"\"\"Implements the required _generate method to handle batch generation.\"\"\"\n            generations = []\n            for prompt in prompts:\n                text = self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n                generations.append([Generation(text=text)])\n\n            return LLMResult(generations=generations)\n\n        @property\n        def _llm_type(self) -> str:\n            return \"custom_llm\"\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_new_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_new_tokens","value":20,"display_name":"Max New Tokens","advanced":false,"dynamic":false,"info":"Maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput","load_from_db":false},"server_url":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"name":"server_url","value":"SERVER_URL","display_name":"Server URL","advanced":false,"dynamic":false,"info":"URL for the custom server.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":"0.1","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Sampling temperature for text generation.","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generates text using a custom LLM server.","icon":"chat","base_classes":["LanguageModel","Message"],"display_name":"Generative LLM","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","server_url","temperature","max_new_tokens"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"MyLLMModel-LbF5I"},"selected":false,"width":384,"height":587,"dragging":false,"positionAbsolute":{"x":1611.5441802765818,"y":-808.7408792282913}},{"id":"Prompt-9F9sY","type":"genericNode","position":{"x":1020.1670619028823,"y":-304.9818641612672},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are an English poetry expert with access to two specialized poetry databases:  \n1) **Similarity Retrieval** — for finding poems that resemble a given poem or poetic text, such as contents, style, mood, or example. This retrieval is based on FAISS indexing of more than 13,000 poems.\n2) **Keyword Retrieval** — for finding poems based on specific keywords, which can be broad themes (e.g., \"nature,\" \"war\") or more specific nouns (e.g., \"tree,\" \"car\"). This retrieval is based on a NetworkX graph of more than 10,000 keywords.\n\n**Task:** Analyze the User Query and select the best retrieval action by choosing one of the options below:\n\n- **[A]: Skip Retrieval** — Select this if the User Query is conversational and doesn’t require poetry retrieval (e.g., greetings, thanks, farewells, or simple acknowledgments like \"Sounds good\" or \"Great idea\").\n\n- **[B]: Similarity Retrieval** — Select this if the User Query includes any poem or poetic language, seeking similarity in style, mood, or content. If you detect such texts, you should always choose this option.\n\n- **[C]: Keyword Retrieval** — Select this if the User Query mentions specific keywords, such as themes (e.g., \"nature,\" \"war\"), nouns (e.g., \"tree,\" \"car\"), and named entities (e.g., \"America\", \"Victoria\").\n\nUser Query: {user_message}  \nAI: My choice is [","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"user_message":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"user_message","display_name":"user_message","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Prompt for retriever selection.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["user_message"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.18"},"id":"Prompt-9F9sY"},"selected":false,"width":384,"height":375,"positionAbsolute":{"x":1020.1670619028823,"y":-304.9818641612672},"dragging":false},{"id":"NullRetriever-7Sz3N","type":"genericNode","position":{"x":2907.169303516998,"y":-617.7301182419611},"data":{"type":"NullRetriever","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass SkipRetrieverComponent(Component):\n    display_name = \"Null Retriever\"\n    description = \"Null retriever returning an empty string (skipped retrival).\"\n    icon = \"space\"\n    name = \"NullRetriever\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"selection\",\n            display_name=\"Retriever Selection\",\n        ),\n        MessageTextInput(\n            name=\"user_input\",\n            display_name=\"User Input\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Retrieved contents\", name=\"retrieved_contents\", method=\"retrieve\"),\n    ]\n\n    def retrieve(self) -> Message:\n        # Return empty string\n        self.status = \"\"\n        return Message(text=\"\")\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"selection":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"selection","value":"","display_name":"Retriever Selection","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"user_input":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"user_input","value":"","display_name":"User Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Null retriever returning an empty string (skipped retrival).","icon":"space","base_classes":["Message"],"display_name":"Null Retriever","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"retrieved_contents","display_name":"Retrieved contents","method":"retrieve","value":"__UNDEFINED__","cache":true}],"field_order":["selection","user_input"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"NullRetriever-7Sz3N"},"selected":false,"width":384,"height":391,"positionAbsolute":{"x":2907.169303516998,"y":-617.7301182419611},"dragging":false},{"id":"FaissRetriever-bBbIN","type":"genericNode","position":{"x":2904.7264381326313,"y":-123.75594862962515},"data":{"type":"FaissRetriever","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass SkipRetrieverComponent(Component):\n    display_name = \"FAISS Retriever\"\n    description = \"FAISS retriever returning retrieved poems.\"\n    icon = \"FAISS\"\n    name = \"FaissRetriever\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"selection\",\n            display_name=\"Retriever Selection\",\n        ),\n        MessageTextInput(\n            name=\"user_input\",\n            display_name=\"User Input\",\n        ),\n        StrInput(\n            name=\"server_url\",\n            display_name=\"Server URL\",\n            info=\"URL for the custom server.\",\n            value=\"https://ABC.loca.lt\",\n        ),\n        IntInput(\n            name=\"k\",\n            display_name=\"Poem Count\",\n            info=\"Number of retrieved poems.\",\n            value=1,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Retrieved contents\", name=\"retrieved_text\", method=\"retrieve\"),\n    ]\n\n    def retrieve(self) -> Message:\n        if \"B\" not in self.selection:\n            # Return empty string\n            self.status = \"\"\n            return Message(text=\"\")\n        \n        payload = {\n            \"text\": self.user_input,\n            \"k\": self.k,\n        }\n        headers = {\n            \"Content-Type\": \"application/json\"\n        }\n        try:\n            response = requests.post(self.server_url + \"/retrieve_faiss\", json=payload, headers=headers)\n            response.raise_for_status()\n            result = response.json()\n            return result.get(\"retrieved_poems\", \"\")\n        except requests.RequestException as e:\n            return f\"Error generating text: {e}\"\n            \n    ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"k","value":1,"display_name":"Poem Count","advanced":false,"dynamic":false,"info":"Number of retrieved poems.","title_case":false,"type":"int","_input_type":"IntInput"},"selection":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"selection","value":"","display_name":"Retriever Selection","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"server_url":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"name":"server_url","value":"SERVER_URL","display_name":"Server URL","advanced":false,"dynamic":false,"info":"URL for the custom server.","title_case":false,"type":"str","_input_type":"StrInput"},"user_input":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"user_input","value":"","display_name":"User Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Similarity-based retriever using FAISS.","icon":"FAISS","base_classes":["Message"],"display_name":"FAISS Retriever","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"retrieved_text","display_name":"Retrieved contents","method":"retrieve","value":"__UNDEFINED__","cache":true}],"field_order":["selection","user_input","server_url","k"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"FaissRetriever-bBbIN"},"selected":false,"width":384,"height":547,"positionAbsolute":{"x":2904.7264381326313,"y":-123.75594862962515},"dragging":false},{"id":"Prompt-4zA54","type":"genericNode","position":{"x":3531.957852436277,"y":-89.59830890758356},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{first}{second}{third}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"first":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"first","display_name":"first","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"second":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"second","display_name":"second","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"third":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"third","display_name":"third","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Merge results","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["first","second","third"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.18"},"id":"Prompt-4zA54"},"selected":false,"width":384,"height":547,"dragging":false,"positionAbsolute":{"x":3531.957852436277,"y":-89.59830890758356}},{"id":"TextOutput-J7qZ9","type":"genericNode","position":{"x":2225.0728951081155,"y":-1180.3926991011162},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Display retriever selection with reasoning.","icon":"type","base_classes":["Message"],"display_name":"Retriever Selection","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"TextOutput-J7qZ9"},"selected":false,"width":384,"height":289,"positionAbsolute":{"x":2225.0728951081155,"y":-1180.3926991011162},"dragging":false},{"id":"SelectionParser-naF1z","type":"genericNode","position":{"x":2254.2355840904424,"y":-651.2684089486299},"data":{"type":"SelectionParser","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import re\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\nclass SelectionParserComponent(Component):\n    display_name = \"Selection Parser\"\n    description = \"Parse retriever selection.\"\n    icon = \"prompts\"\n    name = \"SelectionParser\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"selection\",\n            display_name=\"Selection\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Parsed Selection\", name=\"parsed\", method=\"parse\"),\n    ]\n\n    def parse(self) -> Message:\n        # Retrieve input text from the `selection` input\n        selection_text = self.selection  # Accessing the input value\n        \n        # Check if the first two characters are in the correct format\n        if selection_text[0:2] in [\"A]\", \"B]\", \"C]\"]:\n            choice = selection_text[0]  # Extracts \"A\", \"B\", or \"C\"\n            parsed = f\"{choice}\"  # Formats as [A], [B], or [C]\n        \n        # Fallback: use regex to find any standalone A, B, or C within the text\n        elif re.search(r\"\\bA\\b|\\bB\\b|\\bC\\b\", selection_text):\n            match = re.search(r\"\\bA\\b|\\bB\\b|\\bC\\b\", selection_text)\n            choice = match.group()  # Extracts \"A\", \"B\", or \"C\"\n            parsed = f\"{choice}\"  # Formats as A, B, or C\n        \n        # If no valid format is found, default to \"A\"\n        else:\n            parsed = \"A\"\n\n        self.status = parsed\n        return Message(text=parsed)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"selection":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"selection","value":"","display_name":"Selection","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Parse retriever selection result.","icon":"prompts","base_classes":["Message"],"display_name":"Selection Parser","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"parsed","display_name":"Parsed Selection","method":"parse","value":"__UNDEFINED__","cache":true}],"field_order":["selection"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"SelectionParser-naF1z"},"selected":false,"width":384,"height":289,"dragging":false,"positionAbsolute":{"x":2254.2355840904424,"y":-651.2684089486299}},{"id":"NetworkXRetriever-c9rbn","type":"genericNode","position":{"x":2905.1400944686457,"y":527.4317464093233},"data":{"type":"NetworkXRetriever","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass SkipRetrieverComponent(Component):\n    display_name = \"NetworkX Retriever\"\n    description = \"NetworkX retriever returning retrieved poems.\"\n    icon = \"network\"\n    name = \"NetworkXRetriever\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"selection\",\n            display_name=\"Retriever Selection\",\n        ),\n        MessageTextInput(\n            name=\"user_input\",\n            display_name=\"User Input\",\n        ),\n        StrInput(\n            name=\"server_url\",\n            display_name=\"Server URL\",\n            info=\"URL for the custom server.\",\n            value=\"https://ABC.loca.lt\",\n        ),\n        IntInput(\n            name=\"k\",\n            display_name=\"Poem Count\",\n            info=\"Number of retrieved poems.\",\n            value=1,\n        ),\n        IntInput(\n            name=\"depth\",\n            display_name=\"Depth\",\n            info=\"Depth for graph search.\",\n            value=2,\n        ),\n        FloatInput(\n            name=\"depth_decay\",\n            display_name=\"Depth Decay\",\n            info=\"The factor by which the score decays along depth.\",\n            value=0.5,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Retrieved contents\", name=\"retrieved_text\", method=\"retrieve\"),\n    ]\n\n    def retrieve(self) -> Message:\n        if \"C\" not in self.selection:\n            # Return empty string\n            self.status = \"\"\n            return Message(text=\"\")\n        \n        payload = {\n            \"text\": self.user_input,\n            \"k\": self.k,\n            \"depth\": self.depth,\n            \"depth_decay\": self.depth_decay\n        }\n        headers = {\n            \"Content-Type\": \"application/json\"\n        }\n        try:\n            response = requests.post(self.server_url + \"/retrieve_nx_graph\", json=payload, headers=headers)\n            response.raise_for_status()\n            result = response.json()\n            return result.get(\"retrieved_poems\", \"\")\n        except requests.RequestException as e:\n            return f\"Error generating text: {e}\"\n            \n    ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"depth":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"depth","value":2,"display_name":"Depth","advanced":false,"dynamic":false,"info":"Depth for graph search.","title_case":false,"type":"int","_input_type":"IntInput"},"depth_decay":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"depth_decay","value":0.5,"display_name":"Depth Decay","advanced":false,"dynamic":false,"info":"The factor by which the score decays along depth.","title_case":false,"type":"float","_input_type":"FloatInput"},"k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"k","value":1,"display_name":"Poem Count","advanced":false,"dynamic":false,"info":"Number of retrieved poems.","title_case":false,"type":"int","_input_type":"IntInput"},"selection":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"selection","value":"","display_name":"Retriever Selection","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"},"server_url":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"name":"server_url","value":"SERVER_URL","display_name":"Server URL","advanced":false,"dynamic":false,"info":"URL for the custom server.","title_case":false,"type":"str","_input_type":"StrInput"},"user_input":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"user_input","value":"","display_name":"User Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"NetworkX retriever returning retrieved poems.","icon":"network","base_classes":["Message"],"display_name":"NetworkX Retriever","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"retrieved_text","display_name":"Retrieved contents","method":"retrieve","value":"__UNDEFINED__","cache":true}],"field_order":["selection","user_input","server_url","k","depth","depth_decay"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"NetworkXRetriever-c9rbn"},"selected":false,"width":384,"height":719,"positionAbsolute":{"x":2905.1400944686457,"y":527.4317464093233},"dragging":false},{"id":"TextOutput-ifGSl","type":"genericNode","position":{"x":4154.470247345339,"y":-483.81604581483236},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        text=self.input_value or \"No retrival.\"\n        message = Message(\n            text=text,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Display retrieved poems.","icon":"type","base_classes":["Message"],"display_name":"Retrieved Poems","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"TextOutput-ifGSl"},"selected":false,"width":384,"height":289,"positionAbsolute":{"x":4154.470247345339,"y":-483.81604581483236},"dragging":false},{"id":"Prompt-1JjJU","type":"genericNode","position":{"x":4800.052921583509,"y":1378.4356377989136},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"You are an English poetry professor with deep knowledge of poetic forms, themes, and historical context. Your task is to assist users by providing insightful and relevant responses to their queries about poetry. \n\n{input_retrieved_poems}\n\n---------------------------------\n\n{memory}\n\nAI:\n","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"input_retrieved_poems":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"input_retrieved_poems","display_name":"input_retrieved_poems","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"memory":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","name":"memory","display_name":"memory","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["input_retrieved_poems","memory"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.18"},"id":"Prompt-1JjJU"},"selected":false,"width":384,"height":477,"positionAbsolute":{"x":4800.052921583509,"y":1378.4356377989136},"dragging":false},{"id":"PoemPrompt-vOd5B","type":"genericNode","position":{"x":4128.379748444478,"y":891.4739183489241},"data":{"type":"PoemPrompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\nclass PoemPromptComponent(Component):\n    display_name = \"Poem Prompt\"\n    description = \"Parse retrieved poems into a prompt.\"\n    icon = \"prompts\"\n    name = \"PoemPrompt\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"poems\",\n            display_name=\"Retrieved Poems\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Poem Prompt\", name=\"parsed\", method=\"parse\"),\n    ]\n\n    def parse(self) -> Message:\n        # Access the input poems directly from inputs\n        poems = self.poems\n        \n        # Create the parsed prompt based on whether poems are provided\n        if poems:\n            parsed = (\n                \"Based on the user query, your secretary has retrieved the following poems to help enrich your response. \"\n                \"Use these to provide relevant insights, analyze themes, or add context for the user’s query. \"\n                \"Note: Your secretary will present the original poems to the user, so you do not need to include them verbatim in your answer.\\n\\n\" + poems\n            )\n        else:\n            parsed = \"\"\n        \n        # Optionally set status, if needed for component feedback\n        self.status = parsed\n        \n        return Message(text=parsed)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"poems":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"poems","value":"","display_name":"Retrieved Poems","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Parse retrieved poems into a prompt.","icon":"prompts","base_classes":["Message"],"display_name":"Poem Parser","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"parsed","display_name":"Poem Prompt","method":"parse","value":"__UNDEFINED__","cache":true}],"field_order":["poems"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"PoemPrompt-vOd5B"},"selected":false,"width":384,"height":289,"positionAbsolute":{"x":4128.379748444478,"y":891.4739183489241},"dragging":false},{"id":"MyLLMModel-ZPMfa","type":"genericNode","position":{"x":5388.4590658000325,"y":1373.82627516581},"data":{"type":"MyLLMModel","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import List, Optional, Any\n\nimport requests\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain.schema import LLMResult, Generation\nfrom langchain_core.language_models import BaseLLM\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    StrInput,\n    FloatInput,\n    IntInput,\n)\n\nimport re\n\nclass CustomLLMComponent(LCModelComponent):\n    display_name = \"Generative LLM\"\n    description = \"Generates text using a custom LLM server.\"\n    icon = \"chat\"\n    name = \"MyLLMModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"server_url\",\n            display_name=\"Server URL\",\n            info=\"URL for the custom server.\",\n            value=\"https://ABC.loca.lt\",\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.7,\n            info=\"Sampling temperature for text generation.\",\n            advanced=False,\n        ),\n        IntInput(\n            name=\"max_new_tokens\",\n            display_name=\"Max New Tokens\",\n            value=50,\n            info=\"Maximum number of tokens to generate.\",\n            advanced=False,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:\n        # Instantiate CustomLLM with the appropriate parameters\n        return self.CustomLLM(\n            server_url=self.server_url + \"/generate\",\n            temperature=self.temperature,\n            max_new_tokens=self.max_new_tokens\n        )\n\n    class CustomLLM(BaseLLM):\n        \"\"\"Wrapper class for custom LLM model to comply with the LanguageModel interface.\"\"\"\n\n        # Define fields as class-level variables\n        server_url: str\n        temperature: float\n        max_new_tokens: int\n\n        def _call(\n                self,\n                prompt: str,\n                stop: Optional[List[str]] = None,  # noqa\n                run_manager: Optional[CallbackManagerForLLMRun] = None,  # noqa\n                **kwargs: Any,  # noqa\n        ) -> str:\n            \"\"\"Generate text from the custom LLM model.\"\"\"\n            payload = {\n                \"text\": prompt,\n                \"temperature\": self.temperature,\n                \"max_new_tokens\": self.max_new_tokens\n            }\n            headers = {\n                \"Content-Type\": \"application/json\"\n            }\n            try:\n                response = requests.post(self.server_url, json=payload, headers=headers)\n                response.raise_for_status()\n                result = response.json()\n                return result.get(\"generated_text\", \"No generated text returned.\")\n            except requests.RequestException as e:\n                return f\"Error generating text: {e}\"\n\n        def _generate(\n                self,\n                prompts: List[str],\n                stop: Optional[List[str]] = None,\n                run_manager: Optional[CallbackManagerForLLMRun] = None,\n                **kwargs: Any,\n        ) -> LLMResult:\n            \"\"\"Implements the required _generate method to handle batch generation.\"\"\"\n            generations = []\n            for prompt in prompts:\n                text = self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n                generations.append([Generation(text=text)])\n\n            return LLMResult(generations=generations)\n\n        @property\n        def _llm_type(self) -> str:\n            return \"custom_llm\"\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_new_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"max_new_tokens","value":200,"display_name":"Max New Tokens","advanced":false,"dynamic":false,"info":"Maximum number of tokens to generate.","title_case":false,"type":"int","_input_type":"IntInput","load_from_db":false},"server_url":{"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"name":"server_url","value":"SERVER_URL","display_name":"Server URL","advanced":false,"dynamic":false,"info":"URL for the custom server.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.5,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"Sampling temperature for text generation.","title_case":false,"type":"float","_input_type":"FloatInput","load_from_db":false}},"description":"Generates text using a custom LLM server.","icon":"chat","base_classes":["LanguageModel","Message"],"display_name":"Generative LLM","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","server_url","temperature","max_new_tokens"],"beta":false,"edited":true,"lf_version":"1.0.18"},"id":"MyLLMModel-ZPMfa"},"selected":false,"width":384,"height":587,"positionAbsolute":{"x":5388.4590658000325,"y":1373.82627516581},"dragging":false},{"id":"ChatOutput-1mrXc","type":"genericNode","position":{"x":6431.647626448245,"y":1477.712405521416},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"ChatOutput-1mrXc"},"selected":false,"width":384,"height":289,"positionAbsolute":{"x":6431.647626448245,"y":1477.712405521416},"dragging":false},{"id":"TextOutput-4ZJQ1","type":"genericNode","position":{"x":5311.486599957447,"y":694.3173701277211},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MultilineInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        self.status = self.input_value\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Message"],"display_name":"Final Prompt","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"lf_version":"1.0.18"},"id":"TextOutput-4ZJQ1"},"selected":false,"width":384,"height":289,"dragging":false,"positionAbsolute":{"x":5311.486599957447,"y":694.3173701277211}},{"id":"SingleTurnFilter-vlGWq","type":"genericNode","position":{"x":5917.2926420218655,"y":1466.8555402285958},"data":{"type":"ResponseFilter","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\nclass SingleTurnFilterComponent(Component):\n    display_name = \"Response Filter\"\n    description = \"Removes multi-turn conversation markers (e.g., '\\\\nUser:', '\\\\nAI:') and ensures responses end with complete sentences.\"\n    icon = \"scissors-line-dashed\"\n    name = \"ResponseFilter\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"text\",\n            display_name=\"Text\",\n            info=\"The input text containing conversation segments.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Filtered Response\", name=\"filtered_response\", method=\"filter\"),\n    ]\n\n    def filter(self) -> Message:\n        # Access the input text directly\n        text = self.text\n\n        # Step 1: Remove multi-turn conversation markers, keeping only the initial response\n        filtered_response = text.split(\"\\nUser:\")[0].split(\"\\nAI:\")[0].strip()\n\n        # Step 2: Check for sentence-ending punctuation in the filtered response\n        sentence_endings = [\".\", \"!\", \"?\", \"…\", \"...\"]\n        if not any(filtered_response.endswith(p) for p in sentence_endings):\n            # If no sentence-ending punctuation, truncate to the last complete sentence\n            last_punctuation_index = max(filtered_response.rfind(p) for p in sentence_endings)\n            if last_punctuation_index != -1:\n                filtered_response = filtered_response[:last_punctuation_index + 1]\n        \n        # Set the component status and return the filtered response as a Message\n        self.status = filtered_response\n        return Message(text=filtered_response)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"text":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"text","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The input text containing conversation segments.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Removes multi-turn conversation markers (e.g., '\\nUser:', '\\nAI:') and ensures responses end with complete sentences.","icon":"scissors-line-dashed","base_classes":["Message"],"display_name":"Response Filter","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"filtered_response","display_name":"Filtered Response","method":"filter","value":"__UNDEFINED__","cache":true}],"field_order":["text"],"beta":false,"edited":true},"id":"SingleTurnFilter-vlGWq"},"selected":false,"width":384,"height":321,"dragging":false,"positionAbsolute":{"x":5917.2926420218655,"y":1466.8555402285958}}],"edges":[{"source":"ChatInput-228Fo","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-228Foœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-9F9sY","targetHandle":"{œfieldNameœ:œuser_messageœ,œidœ:œPrompt-9F9sYœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_message","id":"Prompt-9F9sY","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-228Fo","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-228Fo{œdataTypeœ:œChatInputœ,œidœ:œChatInput-228Foœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-Prompt-9F9sY{œfieldNameœ:œuser_messageœ,œidœ:œPrompt-9F9sYœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"Prompt-9F9sY","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-9F9sYœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"MyLLMModel-LbF5I","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œMyLLMModel-LbF5Iœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"MyLLMModel-LbF5I","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-9F9sY","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-9F9sY{œdataTypeœ:œPromptœ,œidœ:œPrompt-9F9sYœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-MyLLMModel-LbF5I{œfieldNameœ:œinput_valueœ,œidœ:œMyLLMModel-LbF5Iœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"MyLLMModel-LbF5I","sourceHandle":"{œdataTypeœ:œMyLLMModelœ,œidœ:œMyLLMModel-LbF5Iœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"TextOutput-J7qZ9","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-J7qZ9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-J7qZ9","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"MyLLMModel","id":"MyLLMModel-LbF5I","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-MyLLMModel-LbF5I{œdataTypeœ:œMyLLMModelœ,œidœ:œMyLLMModel-LbF5Iœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-TextOutput-J7qZ9{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-J7qZ9œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":"","selected":false},{"source":"ChatInput-228Fo","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-228Foœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"NullRetriever-7Sz3N","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œNullRetriever-7Sz3Nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_input","id":"NullRetriever-7Sz3N","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-228Fo","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-228Fo{œdataTypeœ:œChatInputœ,œidœ:œChatInput-228Foœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-NullRetriever-7Sz3N{œfieldNameœ:œuser_inputœ,œidœ:œNullRetriever-7Sz3Nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"ChatInput-228Fo","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-228Foœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"FaissRetriever-bBbIN","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œFaissRetriever-bBbINœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_input","id":"FaissRetriever-bBbIN","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-228Fo","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-228Fo{œdataTypeœ:œChatInputœ,œidœ:œChatInput-228Foœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-FaissRetriever-bBbIN{œfieldNameœ:œuser_inputœ,œidœ:œFaissRetriever-bBbINœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"MyLLMModel-LbF5I","sourceHandle":"{œdataTypeœ:œMyLLMModelœ,œidœ:œMyLLMModel-LbF5Iœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"SelectionParser-naF1z","targetHandle":"{œfieldNameœ:œselectionœ,œidœ:œSelectionParser-naF1zœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"selection","id":"SelectionParser-naF1z","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"MyLLMModel","id":"MyLLMModel-LbF5I","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-MyLLMModel-LbF5I{œdataTypeœ:œMyLLMModelœ,œidœ:œMyLLMModel-LbF5Iœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-SelectionParser-naF1z{œfieldNameœ:œselectionœ,œidœ:œSelectionParser-naF1zœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"SelectionParser-naF1z","sourceHandle":"{œdataTypeœ:œSelectionParserœ,œidœ:œSelectionParser-naF1zœ,œnameœ:œparsedœ,œoutput_typesœ:[œMessageœ]}","target":"NullRetriever-7Sz3N","targetHandle":"{œfieldNameœ:œselectionœ,œidœ:œNullRetriever-7Sz3Nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"selection","id":"NullRetriever-7Sz3N","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"SelectionParser","id":"SelectionParser-naF1z","name":"parsed","output_types":["Message"]}},"id":"reactflow__edge-SelectionParser-naF1z{œdataTypeœ:œSelectionParserœ,œidœ:œSelectionParser-naF1zœ,œnameœ:œparsedœ,œoutput_typesœ:[œMessageœ]}-NullRetriever-7Sz3N{œfieldNameœ:œselectionœ,œidœ:œNullRetriever-7Sz3Nœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"SelectionParser-naF1z","sourceHandle":"{œdataTypeœ:œSelectionParserœ,œidœ:œSelectionParser-naF1zœ,œnameœ:œparsedœ,œoutput_typesœ:[œMessageœ]}","target":"FaissRetriever-bBbIN","targetHandle":"{œfieldNameœ:œselectionœ,œidœ:œFaissRetriever-bBbINœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"selection","id":"FaissRetriever-bBbIN","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"SelectionParser","id":"SelectionParser-naF1z","name":"parsed","output_types":["Message"]}},"id":"reactflow__edge-SelectionParser-naF1z{œdataTypeœ:œSelectionParserœ,œidœ:œSelectionParser-naF1zœ,œnameœ:œparsedœ,œoutput_typesœ:[œMessageœ]}-FaissRetriever-bBbIN{œfieldNameœ:œselectionœ,œidœ:œFaissRetriever-bBbINœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"ChatInput-228Fo","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-228Foœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"NetworkXRetriever-c9rbn","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œNetworkXRetriever-c9rbnœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_input","id":"NetworkXRetriever-c9rbn","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-228Fo","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-228Fo{œdataTypeœ:œChatInputœ,œidœ:œChatInput-228Foœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-NetworkXRetriever-c9rbn{œfieldNameœ:œuser_inputœ,œidœ:œNetworkXRetriever-c9rbnœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"SelectionParser-naF1z","sourceHandle":"{œdataTypeœ:œSelectionParserœ,œidœ:œSelectionParser-naF1zœ,œnameœ:œparsedœ,œoutput_typesœ:[œMessageœ]}","target":"NetworkXRetriever-c9rbn","targetHandle":"{œfieldNameœ:œselectionœ,œidœ:œNetworkXRetriever-c9rbnœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"selection","id":"NetworkXRetriever-c9rbn","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"SelectionParser","id":"SelectionParser-naF1z","name":"parsed","output_types":["Message"]}},"id":"reactflow__edge-SelectionParser-naF1z{œdataTypeœ:œSelectionParserœ,œidœ:œSelectionParser-naF1zœ,œnameœ:œparsedœ,œoutput_typesœ:[œMessageœ]}-NetworkXRetriever-c9rbn{œfieldNameœ:œselectionœ,œidœ:œNetworkXRetriever-c9rbnœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"NullRetriever-7Sz3N","sourceHandle":"{œdataTypeœ:œNullRetrieverœ,œidœ:œNullRetriever-7Sz3Nœ,œnameœ:œretrieved_contentsœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-4zA54","targetHandle":"{œfieldNameœ:œfirstœ,œidœ:œPrompt-4zA54œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"first","id":"Prompt-4zA54","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"NullRetriever","id":"NullRetriever-7Sz3N","name":"retrieved_contents","output_types":["Message"]}},"id":"reactflow__edge-NullRetriever-7Sz3N{œdataTypeœ:œNullRetrieverœ,œidœ:œNullRetriever-7Sz3Nœ,œnameœ:œretrieved_contentsœ,œoutput_typesœ:[œMessageœ]}-Prompt-4zA54{œfieldNameœ:œfirstœ,œidœ:œPrompt-4zA54œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"FaissRetriever-bBbIN","sourceHandle":"{œdataTypeœ:œFaissRetrieverœ,œidœ:œFaissRetriever-bBbINœ,œnameœ:œretrieved_textœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-4zA54","targetHandle":"{œfieldNameœ:œsecondœ,œidœ:œPrompt-4zA54œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"second","id":"Prompt-4zA54","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"FaissRetriever","id":"FaissRetriever-bBbIN","name":"retrieved_text","output_types":["Message"]}},"id":"reactflow__edge-FaissRetriever-bBbIN{œdataTypeœ:œFaissRetrieverœ,œidœ:œFaissRetriever-bBbINœ,œnameœ:œretrieved_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-4zA54{œfieldNameœ:œsecondœ,œidœ:œPrompt-4zA54œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"NetworkXRetriever-c9rbn","sourceHandle":"{œdataTypeœ:œNetworkXRetrieverœ,œidœ:œNetworkXRetriever-c9rbnœ,œnameœ:œretrieved_textœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-4zA54","targetHandle":"{œfieldNameœ:œthirdœ,œidœ:œPrompt-4zA54œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"third","id":"Prompt-4zA54","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"NetworkXRetriever","id":"NetworkXRetriever-c9rbn","name":"retrieved_text","output_types":["Message"]}},"id":"reactflow__edge-NetworkXRetriever-c9rbn{œdataTypeœ:œNetworkXRetrieverœ,œidœ:œNetworkXRetriever-c9rbnœ,œnameœ:œretrieved_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-4zA54{œfieldNameœ:œthirdœ,œidœ:œPrompt-4zA54œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":"","selected":false},{"source":"Prompt-4zA54","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-4zA54œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"TextOutput-ifGSl","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-ifGSlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-ifGSl","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-4zA54","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-4zA54{œdataTypeœ:œPromptœ,œidœ:œPrompt-4zA54œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-TextOutput-ifGSl{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-ifGSlœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":"","selected":false},{"source":"Prompt-4zA54","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-4zA54œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"PoemPrompt-vOd5B","targetHandle":"{œfieldNameœ:œpoemsœ,œidœ:œPoemPrompt-vOd5Bœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"poems","id":"PoemPrompt-vOd5B","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-4zA54","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-4zA54{œdataTypeœ:œPromptœ,œidœ:œPrompt-4zA54œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-PoemPrompt-vOd5B{œfieldNameœ:œpoemsœ,œidœ:œPoemPrompt-vOd5Bœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"Prompt-1JjJU","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-1JjJUœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"MyLLMModel-ZPMfa","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œMyLLMModel-ZPMfaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"MyLLMModel-ZPMfa","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-1JjJU","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-1JjJU{œdataTypeœ:œPromptœ,œidœ:œPrompt-1JjJUœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-MyLLMModel-ZPMfa{œfieldNameœ:œinput_valueœ,œidœ:œMyLLMModel-ZPMfaœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","animated":false},{"source":"PoemPrompt-vOd5B","sourceHandle":"{œdataTypeœ:œPoemPromptœ,œidœ:œPoemPrompt-vOd5Bœ,œnameœ:œparsedœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-1JjJU","targetHandle":"{œfieldNameœ:œinput_retrieved_poemsœ,œidœ:œPrompt-1JjJUœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_retrieved_poems","id":"Prompt-1JjJU","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"PoemPrompt","id":"PoemPrompt-vOd5B","name":"parsed","output_types":["Message"]}},"id":"reactflow__edge-PoemPrompt-vOd5B{œdataTypeœ:œPoemPromptœ,œidœ:œPoemPrompt-vOd5Bœ,œnameœ:œparsedœ,œoutput_typesœ:[œMessageœ]}-Prompt-1JjJU{œfieldNameœ:œinput_retrieved_poemsœ,œidœ:œPrompt-1JjJUœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Memory-5frTm","sourceHandle":"{œdataTypeœ:œMemoryœ,œidœ:œMemory-5frTmœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-1JjJU","targetHandle":"{œfieldNameœ:œmemoryœ,œidœ:œPrompt-1JjJUœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"memory","id":"Prompt-1JjJU","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Memory","id":"Memory-5frTm","name":"messages_text","output_types":["Message"]}},"id":"reactflow__edge-Memory-5frTm{œdataTypeœ:œMemoryœ,œidœ:œMemory-5frTmœ,œnameœ:œmessages_textœ,œoutput_typesœ:[œMessageœ]}-Prompt-1JjJU{œfieldNameœ:œmemoryœ,œidœ:œPrompt-1JjJUœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"Prompt-1JjJU","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-1JjJUœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"TextOutput-4ZJQ1","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-4ZJQ1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-4ZJQ1","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-1JjJU","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-1JjJU{œdataTypeœ:œPromptœ,œidœ:œPrompt-1JjJUœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-TextOutput-4ZJQ1{œfieldNameœ:œinput_valueœ,œidœ:œTextOutput-4ZJQ1œ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""},{"source":"MyLLMModel-ZPMfa","sourceHandle":"{œdataTypeœ:œMyLLMModelœ,œidœ:œMyLLMModel-ZPMfaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"SingleTurnFilter-vlGWq","targetHandle":"{œfieldNameœ:œtextœ,œidœ:œSingleTurnFilter-vlGWqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"text","id":"SingleTurnFilter-vlGWq","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"MyLLMModel","id":"MyLLMModel-ZPMfa","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-MyLLMModel-ZPMfa{œdataTypeœ:œMyLLMModelœ,œidœ:œMyLLMModel-ZPMfaœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-SingleTurnFilter-vlGWq{œfieldNameœ:œtextœ,œidœ:œSingleTurnFilter-vlGWqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","animated":false,"className":""}],"viewport":{"x":-4422.36354362072,"y":-1213.8213416389244,"zoom":0.880091397311133}},"description":"","name":"Poetry Memory Chatbot","last_tested_version":"1.0.18","endpoint_name":null,"is_component":false}