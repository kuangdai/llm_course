{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is the first in a series aimed at optimizing large language model (LLM) deployment for efficient and scalable inference. We focus on inference using models from **Hugging Face**, a key source of cutting-edge LLMs, starting with the **Llama 3** model. We also explore **quantization**, which reduces memory usage, allowing models to run efficiently on smaller hardware.\n",
    "\n",
    "Additionally, we introduce **vLLM**, a specialized framework for improving inference speed and memory handling, particularly useful in high-load scenarios involving large batches or long sequences. This notebook sets the foundation for future installments in the series, where we'll further refine performance strategies for various LLM use cases.\n",
    "\n",
    "Finally, we build a simple **chatbot** as our first application, showcasing how these techniques can be applied in a real-world setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlBmfOVOoYaV",
    "outputId": "51581743-08bf-40dd-df79-c20e6f39f9ca"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import vllm\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4VS8C_0t4EJ"
   },
   "source": [
    "# Loading Tokenizer and Model\n",
    "\n",
    "### Using LLaMA 3\n",
    "\n",
    "#### Conditions\n",
    "\n",
    "Llama 3 can be used for commercial products, but there are certain requirements and restrictions to follow:\n",
    "\n",
    "1. **Attribution**:  \n",
    "   You must provide a clear and prominent acknowledgment, such as \"Built with Meta Llama 3,\" in all relevant user interfaces, documentation, and webpages.\n",
    "\n",
    "2. **User Threshold**:  \n",
    "   If your product or service utilizing Llama 3 exceeds **700 million monthly active users**, you must obtain a separate, specific license from Meta.\n",
    "\n",
    "3. **Restrictions on Enhancing Other Models**:  \n",
    "   Llama 3 materials or outputs cannot be used to improve or train any other large language models outside the Llama family.\n",
    "\n",
    "4. **Compliance**:  \n",
    "   Users must ensure compliance with applicable laws and regulations, such as GDPR and trade compliance laws.\n",
    "\n",
    "In conclusion, Llama 3 offers great flexibility for commercial applications, provided you adhere to these licensing terms and restrictions. See [Llama 3 Overview](https://ai.meta.com/static-resource/july-responsible-use-guide).\n",
    "\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. **Apply for model access**: Visit [Llama-3-8B-Instruct on Hugging Face](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) to request access to the model. Please note that it may take a few days for your application to be approved. Once approved, you will see the following message on the website:\n",
    "\n",
    "    * **Gated model** You have been granted access to this model\n",
    "   \n",
    "2. **Create your Hugging Face Access Key**: Go to your [Hugging Face settings](https://huggingface.co/settings/tokens) to create an access token. When creating the token, ensure you check the box:\n",
    "\n",
    "    * `Read access to contents of all public gated repos you can access` under **Permissions**.\n",
    "\n",
    "3. **Provide your Hugging Face Access Key**: Once you have your access token, paste it into `api_keys.json` to authenticate the notebook with Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIjKW_57pjjO"
   },
   "outputs": [],
   "source": [
    "# Read HF_ACCESS_KEY into hf_access_key\n",
    "with open(\"api_keys.json\", \"r\") as file:\n",
    "    hf_access_key = json.load(file).get(\"HF_ACCESS_KEY\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(hf_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGtNsN8DRjOK"
   },
   "source": [
    "### Quantization\n",
    "\n",
    "Suppose you have access to **16 GB of GPU memory**, which is insufficient to load the entire LLaMA model at once. To complete inference, Hugging Face will dynamically move parts of the model onto the GPU during runtime, which will cause the inference to become **extremely slow**.\n",
    "\n",
    "To address this limitation, we **quantize** the model using Hugging Face's `bitsandbytes` library. This approach significantly reduces GPU memory consumption, enabling faster inference without needing to load the entire model into GPU memory at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9prcxTERlZz"
   },
   "outputs": [],
   "source": [
    "# Create a BitsAndBytesConfig for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Change this to `False` to disable quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Optional for performance\n",
    "    bnb_4bit_quant_type='nf4',  # Normal floating-point 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Set compute dtype to float16 for faster inference\n",
    ")\n",
    "\n",
    "# Model name--you can change to many huggingface models\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBrEpszVTfDS"
   },
   "source": [
    "Now, let's load the tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2bb3efd6d1794a01b66c27b867409c2d",
      "93a79789eaf4404cbb1e66680275d347",
      "f4a1e41696564260af9edf4c87f120ba",
      "ab99f154499641c7b16a724d09409956",
      "4f8eae8335f147bbab1107f619f30d24",
      "3f43a26e911f4f46b9802186942fe408",
      "89c6f44f7f2c4adca32af7d6f3a58452",
      "f5cb7c7eb6ef4069bd13099582cf5453",
      "13d2c8aca673481184654e1af26a2b73",
      "0d6eb16f5f5a4a5ca7d2f2caa21d4b5d",
      "801549846c88431792724812adc8e408"
     ]
    },
    "id": "4HXJBeHCohOX",
    "outputId": "ff803656-8755-400f-8a0a-902c6365d6be"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ED02SzHZTvNg"
   },
   "source": [
    "Let’s check the GPU information and verify that all parts of the model are loaded onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDwKD8171SYP",
    "outputId": "1c6f5f2c-c12f-4958-f0e6-bb95e0b55c71",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clear_info_gpu():\n",
    "    \"\"\" Clear GPU cache and print info \"\"\"\n",
    "    # Clear the memory cache on the GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    # Collect garbage to ensure all references are removed\n",
    "    gc.collect()\n",
    "    # Ensure all CUDA operations are finished before clearing memory (optional)\n",
    "    torch.cuda.synchronize()\n",
    "    # Print GPU info\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1024 ** 3\n",
    "    free_memory = total_memory - allocated_memory\n",
    "    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Allocated GPU memory: {allocated_memory:.2f} GB\")\n",
    "    print(f\"Free GPU memory: {free_memory:.2f} GB\")\n",
    "\n",
    "\n",
    "# Check GPU info\n",
    "clear_info_gpu()\n",
    "\n",
    "# Check the device of each module of the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is on device: {param.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV1-qVCnUWkf"
   },
   "source": [
    "# Inference\n",
    "\n",
    "Let's try the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VGx93sN0pAv9",
    "outputId": "c9e14a78-20ef-4f5d-9c5f-5ec68dfcf10a"
   },
   "outputs": [],
   "source": [
    "# Generate chat response\n",
    "def generate_response(prompt, max_new_tokens=30, temperature=0.2, num_beams=1):\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the output ids\n",
    "    outputs = model.generate(inputs.input_ids,\n",
    "                             attention_mask=inputs['attention_mask'],  # Avoid warning\n",
    "                             pad_token_id=tokenizer.eos_token_id,  # Avoid warning\n",
    "                             max_new_tokens=max_new_tokens,  # Length of generation\n",
    "                             temperature=temperature,  # Temperature for randomness\n",
    "                             num_beams=num_beams  # Number of beams\n",
    "                             )\n",
    "\n",
    "    # Decode the output ids to a string\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Try it\n",
    "generate_response(\"Hi mate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NBJlpV3X_pj"
   },
   "source": [
    "### Key Parameters in `model.generate()`\n",
    "\n",
    "There are numerous arguments for `model.generate()`. Below, we highlight the most useful ones.\n",
    "\n",
    "#### **Length Control**\n",
    "- **`max_length`**: Specifies the maximum number of tokens for the entire sequence, including both input tokens (from the prompt) and generated tokens. The model stops generating once the total number of tokens reaches this limit.\n",
    "- **`max_new_tokens`**: Defines the maximum number of new tokens that the model can generate, excluding the tokens from the input prompt. The model will generate up to this many tokens after receiving the input.\n",
    "- **`eos_token_id`**: The ID of the end-of-sequence (EOS) token. The generation will stop once the model generates this token, marking the end of the sequence.\n",
    "\n",
    "#### **Diversity and Quality**\n",
    "- **`temperature`**: Controls the randomness of predictions. Lower values (e.g., 0.7) make the model more deterministic, while higher values (e.g., 1.0 or above) increase randomness, making the outputs more diverse.\n",
    "- **`top_k`**: Limits the next token selection to the top `k` most likely tokens. A higher value allows for more variety in the generated text, while a lower value makes it more deterministic.\n",
    "- **`top_p` (nucleus sampling)**: Limits token selection to tokens with a cumulative probability of `p`. This ensures that only the top `p` percent of the probability mass is considered, promoting diverse but controlled generation.\n",
    "- **`do_sample`**: Enables random sampling of tokens instead of greedy decoding (which selects the highest-probability token). This is essential for generating diverse outputs.\n",
    "- **`num_beams`**: The number of beams for beam search. Higher values explore more possibilities during generation, leading to better outputs but at the cost of increased computation.\n",
    "\n",
    "![beam](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "#### **Repetition and Token Constraints**\n",
    "- **`repetition_penalty`**: Penalizes repeated tokens, discouraging the model from generating repetitive sequences. A value greater than 1.0 reduces the likelihood of repeating the same token.\n",
    "- **`no_repeat_ngram_size`**: Prevents repetition of n-grams of a specified size. For example, `no_repeat_ngram_size=3` ensures that trigrams do not repeat in the generated output.\n",
    "\n",
    "#### **Output Control**\n",
    "- **`num_return_sequences`**: The number of different sequences to generate. For example, `num_return_sequences=3` generates three separate outputs from the same prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Batch inference\n",
    "\n",
    "Now, we enhance our `generate_response()` function to support batch inference, which is crucial for serving multiple users in production environments. Additionally, we add functionality to measure the runtime of the inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJ5Yd6M8Oj5o"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompts, max_new_tokens=30, temperature=0.2, num_beams=1, measure_time=False):\n",
    "    # Check if input is a single prompt or batch of prompts\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]  # Convert single prompt to a list for batch processing\n",
    "\n",
    "    # Tokenize the batch of prompts (single or multiple)\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "    # Start time measurement for model.generate() if requested\n",
    "    start_time = time.time() if measure_time else None\n",
    "\n",
    "    # Generate responses for the batch\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs['attention_mask'],  # Avoid warning\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Avoid warning\n",
    "        max_new_tokens=max_new_tokens,  # Length of generation\n",
    "        temperature=temperature,  # Temperature for randomness\n",
    "        num_beams=num_beams  # Number of beams\n",
    "    )\n",
    "\n",
    "    # Measure time after generation\n",
    "    if measure_time:\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "    else:\n",
    "        runtime = None\n",
    "\n",
    "    # Decode the batch of generated outputs\n",
    "    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    # If the original input was a single prompt, return a single string, not a list\n",
    "    if len(prompts) == 1:\n",
    "        responses = responses[0]\n",
    "\n",
    "    # Return both the response and runtime if time measurement was requested\n",
    "    return (responses, runtime) if measure_time else responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For batched inference, padding is necessary to ensure that all input sequences in a batch are of the same length. This allows the model to process multiple inputs in parallel. To set up padding correctly, we need to configure the tokenizer to handle padding. Specifically, we can assign a padding token (typically the `eos_token`) and set the padding side to ensure proper alignment of input sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the pad token to the eos token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's create a multi-input task, and measure its runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Batch input\n",
    "many_prompts = [\n",
    "    \"Once upon a time in a distant land...\",\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Generate a story about a robot in the future.\",\n",
    "    \"How do you bake a chocolate cake?\"\n",
    "]\n",
    "\n",
    "# Batch inference\n",
    "many_responses, t = generate_response(many_prompts, measure_time=True)\n",
    "\n",
    "# Print results\n",
    "for res in many_responses:\n",
    "    print(\"---------------\")\n",
    "    print(res)\n",
    "print(\"\\n\\nRuntime before acceleration:\", t)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Acceleration\n",
    "\n",
    "In this section, we utilize **vLLM** to optimize inference through **paged attention**, as described in [this paper](https://arxiv.org/abs/2309.06180). The key idea of paged attention is to enhance memory efficiency by managing the key-value (KV) cache in a way that minimizes memory fragmentation. This is achieved by allocating memory in smaller \"pages\" instead of reserving large, contiguous memory blocks, which reduces memory fragmentation and allows for more flexible and efficient GPU memory usage. Through this approach, vLLM greatly improves inference throughput, particularly for batched or parallel requests, enabling Large Language Models (LLMs) to handle multiple sequences simultaneously while maintaining high performance on limited GPU resources."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you encounter an error, it is likely that vLLM is not properly installed. In this case, set the following flag to `True` to skip all vLLM-related cells."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SKIP_VLLM_CELLS = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before proceeding, we release the GPU memory by deleting the previous model, ensuring efficient usage of resources on a small GPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VLLM_CELLS:\n",
    "    # Delete the model and free its GPU memory\n",
    "    try:\n",
    "        del model  # Deletes the model object\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    clear_info_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we wrap our Llama 3 model in the `vllm.LLM` wrapper. The `gpu_memory_utilization` parameter is essential, setting the proportion of GPU memory (between 0 and 1) allocated for model weights, activations, and the KV cache. Higher values increase the available KV cache size, enhancing throughput by allowing for more efficient processing of larger sequences or batches. However, setting this value too high risks out-of-memory (OOM) errors, so it’s crucial to balance utilization based on the specific memory capacity of the GPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_VLLM_CELLS:\n",
    "    # Use vllm to load model\n",
    "    model = vllm.LLM(model=model_name,\n",
    "                     skip_tokenizer_init=True,\n",
    "                     quantization=\"bitsandbytes\", load_format=\"bitsandbytes\",\n",
    "                     dtype=\"half\", device=\"cuda\",\n",
    "                     gpu_memory_utilization=0.5,\n",
    "                     max_seq_len_to_capture=1024)\n",
    "\n",
    "    # Check GPU info\n",
    "    clear_info_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, modify the generate function for `vllm.LLM`. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_vllm(prompts, max_new_tokens=50, temperature=0.2, measure_time=False):\n",
    "    # Check if input is a single prompt or batch of prompts\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]  # Convert single prompt to a list for batch processing\n",
    "\n",
    "    # Tokenize the batch of prompts (single or multiple)\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    inputs = [vllm.TokensPrompt(prompt_token_ids=inp) for inp in inputs.data[\"input_ids\"].tolist()]\n",
    "\n",
    "    # Start time measurement for model.generate() if requested\n",
    "    start_time = time.time() if measure_time else None\n",
    "\n",
    "    # Generate responses for the batch\n",
    "    outputs = model.generate(\n",
    "        prompts=inputs,\n",
    "        sampling_params=vllm.SamplingParams(max_tokens=max_new_tokens, temperature=temperature),\n",
    "    )\n",
    "\n",
    "    # Measure time after generation\n",
    "    if measure_time:\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "    else:\n",
    "        runtime = None\n",
    "\n",
    "    # Decode the batch of generated outputs\n",
    "    responses = [tokenizer.decode(output.outputs[0].token_ids, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    # If the original input was a single prompt, return a single string, not a list\n",
    "    if len(prompts) == 1:\n",
    "        responses = responses[0]\n",
    "\n",
    "    # Return both the response and runtime if time measurement was requested\n",
    "    return (responses, runtime) if measure_time else responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Fk2goXdObF-"
   },
   "outputs": [],
   "source": [
    "if not SKIP_VLLM_CELLS:\n",
    "    # Batch inference\n",
    "    many_responses, t = generate_response_vllm(many_prompts, measure_time=True)\n",
    "\n",
    "    # Print results\n",
    "    for res in many_responses:\n",
    "        print(\"---------------\")\n",
    "        print(res)\n",
    "    print(\"\\n\\nRuntime before acceleration:\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zs-iF825Dvq"
   },
   "source": [
    "### Results\n",
    "\n",
    "The results indicate that while vLLM significantly increases GPU memory usage, it does not provide faster inference speeds in our specific setup.\n",
    "\n",
    "- **Increased memory usage**: This occurs because vLLM pre-allocates GPU memory to optimize execution. The pre-allocation is designed for handling large-scale workloads efficiently, which can lead to higher memory usage even when dealing with smaller models or batches.\n",
    "  \n",
    "- **No speedup observed**: The similar inference speeds are likely due to our small batch size and short sequence lengths. vLLM's paged attention is optimized for large batches and long sequences, where it can reduce latency by efficiently managing memory. For small workloads, the overhead of this optimization outweighs the potential speed gains.\n",
    "\n",
    "#### When to use vLLM\n",
    "\n",
    "You will benefit most from using vLLM if:\n",
    "\n",
    "- You have a large model (e.g., non-quantized models that require significant memory) and GPU (or GPU-cluster).\n",
    "- You are working with large batch sizes and/or long sequence lengths, where vLLM's paged attention can significantly improve speed and resource utilization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "In this section, we build a simple chatbot as our first application. \n",
    "\n",
    "First, let's re-create the model using `from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model and free its GPU memory\n",
    "try:\n",
    "    del model  # Deletes the model object\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "clear_info_gpu()\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs do not retain memory between interactions. To illustrate this, consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_response(\"What are the first three colors in the rainbow?\"))\n",
    "print(\"-------\")\n",
    "print(generate_response(\"What are the rest?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the model fails to interpret what \"the rest\" refers to. This is because LLMs process each query independently and do not retain the context of previous interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the limitation of LLMs not retaining memory between interactions, we can implement a **streamed history**. This technique involves maintaining a conversation history by appending previous user inputs and model responses to a single prompt. By doing so, we simulate context retention, allowing the model to handle follow-up questions more effectively.\n",
    "\n",
    "We will also provide a **world prompt** to set the stage for the conversation, leading the chatbot to respond in a specific context or theme. This world prompt acts as an overarching guide for the interaction, helping maintain consistency throughout the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give your bot a name\n",
    "bot_name = \"Rachel\"\n",
    "\n",
    "# Initial world prompt to guide the conversation\n",
    "world_prompt = f\"\"\"\n",
    "Your name is {bot_name}. \n",
    "You are a knowledgeable assistant, capable of answering questions on a wide variety of topics. \n",
    "Assist the user with information, advice, or explanations based on their queries. \n",
    "You only generate your own answer. Do not continue to generate user input.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the conversation history with the world prompt\n",
    "history = world_prompt\n",
    "\n",
    "print(f\"Chatbot {bot_name} initialized. Type 'exit' to end the conversation.\\n\")\n",
    "\n",
    "# Loop for continuous user input\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"[You] \")\n",
    "\n",
    "    # Exit condition\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Ending the conversation.\")\n",
    "        break\n",
    "\n",
    "    # Append the new user input to the history\n",
    "    final_prompts = f\"{history}\\nUser: {user_input}\\n{bot_name}:\"\n",
    "\n",
    "    # Generate chatbot response using the existing generate_response function\n",
    "    response = generate_response(final_prompts, max_new_tokens=100)\n",
    "\n",
    "    # Exclude the input tokens (prompt) from the response by slicing out the history part\n",
    "    generated_text = response[len(final_prompts):]  # Only take the newly generated part\n",
    "\n",
    "    # Only take the first line because further conversation may be generated\n",
    "    if \"\\n\" in generated_text:\n",
    "        generated_text = generated_text.split(\"\\n\")[0]\n",
    "\n",
    "    # Print the chatbot response without including the prompt\n",
    "    print(f\"[{bot_name}] {generated_text}\\n\")\n",
    "\n",
    "    # Update conversation history with the user input and chatbot response\n",
    "    history = f\"{history}\\nUser: {user_input}\\n{bot_name}: {generated_text}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our chatbot works as expected. However, as the conversation continues, the history accumulates, leading to **an increasingly larger sequence length**. This will gradually slow down inference and, eventually, hit the token limit of the model. To address this, we can apply advanced prompt engineering techniques, such as conversation chaining (e.g., in LangChain), which helps manage context by retaining only the most relevant parts of the conversation while discarding older, less relevant exchanges. This keeps the sequence length manageable and ensures efficient inference over time."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More Exercises\n",
    "\n",
    "* Experiment with different models available on Hugging Face to compare their performance and suitability for various tasks.\n",
    "* Explore additional parameters in `model.generate()`, such as `temperature`, `top_k`, and `num_beams`, to understand their impact on the quality and creativity of responses, as well as the inference speed.\n",
    "* Enhance the chatbot's memory management: when the chat history exceeds a certain length, prompt the LLM to summarize the conversation into a concise paragraph. This will help maintain context while reducing token usage and improving efficiency. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d6eb16f5f5a4a5ca7d2f2caa21d4b5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13d2c8aca673481184654e1af26a2b73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2bb3efd6d1794a01b66c27b867409c2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93a79789eaf4404cbb1e66680275d347",
       "IPY_MODEL_f4a1e41696564260af9edf4c87f120ba",
       "IPY_MODEL_ab99f154499641c7b16a724d09409956"
      ],
      "layout": "IPY_MODEL_4f8eae8335f147bbab1107f619f30d24"
     }
    },
    "3f43a26e911f4f46b9802186942fe408": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f8eae8335f147bbab1107f619f30d24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "801549846c88431792724812adc8e408": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89c6f44f7f2c4adca32af7d6f3a58452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93a79789eaf4404cbb1e66680275d347": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f43a26e911f4f46b9802186942fe408",
      "placeholder": "​",
      "style": "IPY_MODEL_89c6f44f7f2c4adca32af7d6f3a58452",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "ab99f154499641c7b16a724d09409956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d6eb16f5f5a4a5ca7d2f2caa21d4b5d",
      "placeholder": "​",
      "style": "IPY_MODEL_801549846c88431792724812adc8e408",
      "value": " 2/2 [00:47&lt;00:00, 23.18s/it]"
     }
    },
    "f4a1e41696564260af9edf4c87f120ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5cb7c7eb6ef4069bd13099582cf5453",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13d2c8aca673481184654e1af26a2b73",
      "value": 2
     }
    },
    "f5cb7c7eb6ef4069bd13099582cf5453": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
