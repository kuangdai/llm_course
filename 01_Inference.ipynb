{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is the first in a series aimed at optimizing large language model (LLM) deployment for efficient and scalable inference. We focus on inference using models from **Hugging Face**, a key source of cutting-edge LLMs, starting with the **Llama 3** model. We also explore **quantization**, which reduces memory usage, allowing models to run efficiently on smaller hardware.\n",
    "\n",
    "Additionally, we introduce **vLLM**, a specialized framework for improving inference speed and memory handling, particularly useful in high-load scenarios involving large batches or long sequences. This notebook sets the foundation for future installments in the series, where we'll further refine performance strategies for various LLM use cases.\n",
    "\n",
    "Finally, we build a simple **chatbot** as our first application, showcasing how these techniques can be applied in a real-world setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlBmfOVOoYaV",
    "outputId": "51581743-08bf-40dd-df79-c20e6f39f9ca"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import vllm\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4VS8C_0t4EJ"
   },
   "source": [
    "# Loading Tokenizer and Model\n",
    "\n",
    "### Using LLaMA 3\n",
    "\n",
    "#### Conditions\n",
    "\n",
    "Llama 3 can be used for commercial products, but there are certain requirements and restrictions to follow:\n",
    "\n",
    "1. **Attribution**:  \n",
    "   You must provide a clear and prominent acknowledgment, such as \"Built with Meta Llama 3,\" in all relevant user interfaces, documentation, and webpages.\n",
    "\n",
    "2. **User Threshold**:  \n",
    "   If your product or service utilizing Llama 3 exceeds **700 million monthly active users**, you must obtain a separate, specific license from Meta.\n",
    "\n",
    "3. **Restrictions on Enhancing Other Models**:  \n",
    "   Llama 3 materials or outputs cannot be used to improve or train any other large language models outside the Llama family.\n",
    "\n",
    "4. **Compliance**:  \n",
    "   Users must ensure compliance with applicable laws and regulations, such as GDPR and trade compliance laws.\n",
    "\n",
    "In conclusion, Llama 3 offers great flexibility for commercial applications, provided you adhere to these licensing terms and restrictions. See [Llama 3 Overview](https://ai.meta.com/static-resource/july-responsible-use-guide).\n",
    "\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. **Apply for model access**: Visit [Llama-3-8B-Instruct on Hugging Face](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) to request access to the model. Please note that it may take a few days for your application to be approved. Once approved, you will see the following message on the website:\n",
    "\n",
    "    * **Gated model** You have been granted access to this model\n",
    "   \n",
    "2. **Create your Hugging Face Access Key**: Go to your [Hugging Face settings](https://huggingface.co/settings/tokens) to create an access token. When creating the token, ensure you check the box:\n",
    "\n",
    "    * `Read access to contents of all public gated repos you can access` under **Permissions**.\n",
    "\n",
    "3. **Provide your Hugging Face Access Key**: Once you have your access token, paste it into `api_keys.json` to authenticate the notebook with Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iIjKW_57pjjO"
   },
   "outputs": [],
   "source": [
    "# Read HF_ACCESS_KEY into hf_access_key\n",
    "with open(\"api_keys.json\", \"r\") as file:\n",
    "    hf_access_key = json.load(file).get(\"HF_ACCESS_KEY\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(hf_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGtNsN8DRjOK"
   },
   "source": [
    "### Quantization\n",
    "\n",
    "Suppose you have access to **16 GB of GPU memory**, which is insufficient to load the entire LLaMA model at once. To complete inference, Hugging Face will dynamically move parts of the model onto the GPU during runtime, which will cause the inference to become **extremely slow**.\n",
    "\n",
    "To address this limitation, we **quantize** the model using Hugging Face's `bitsandbytes` library. This approach significantly reduces GPU memory consumption, enabling faster inference without needing to load the entire model into GPU memory at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K9prcxTERlZz"
   },
   "outputs": [],
   "source": [
    "# Create a BitsAndBytesConfig for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Change this to `False` to disable quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Optional for performance\n",
    "    bnb_4bit_quant_type='nf4',  # Normal floating-point 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Set compute dtype to float16 for faster inference\n",
    ")\n",
    "\n",
    "# Model name--you can change to many huggingface models\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBrEpszVTfDS"
   },
   "source": [
    "Now, let's load the tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2bb3efd6d1794a01b66c27b867409c2d",
      "93a79789eaf4404cbb1e66680275d347",
      "f4a1e41696564260af9edf4c87f120ba",
      "ab99f154499641c7b16a724d09409956",
      "4f8eae8335f147bbab1107f619f30d24",
      "3f43a26e911f4f46b9802186942fe408",
      "89c6f44f7f2c4adca32af7d6f3a58452",
      "f5cb7c7eb6ef4069bd13099582cf5453",
      "13d2c8aca673481184654e1af26a2b73",
      "0d6eb16f5f5a4a5ca7d2f2caa21d4b5d",
      "801549846c88431792724812adc8e408"
     ]
    },
    "id": "4HXJBeHCohOX",
    "outputId": "ff803656-8755-400f-8a0a-902c6365d6be"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5d38f59f28401eb0686b2c4e2362be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ED02SzHZTvNg"
   },
   "source": [
    "Let’s check the GPU information and verify that all parts of the model are loaded onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDwKD8171SYP",
    "outputId": "1c6f5f2c-c12f-4958-f0e6-bb95e0b55c71",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 39.39 GB\n",
      "Allocated GPU memory: 5.31 GB\n",
      "Free GPU memory: 34.08 GB\n",
      "model.embed_tokens.weight is on device: cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.0.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.0.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.0.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.1.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.1.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.1.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.2.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.2.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.2.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.3.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.3.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.3.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.4.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.4.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.4.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.5.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.5.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.5.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.6.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.6.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.6.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.7.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.7.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.7.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.8.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.8.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.8.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.8.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.8.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.8.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.8.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.8.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.9.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.9.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.9.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.9.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.9.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.9.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.9.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.9.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.9.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.10.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.10.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.10.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.10.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.10.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.10.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.10.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.10.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.10.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.11.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.11.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.11.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.11.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.11.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.11.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.11.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.11.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.11.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.12.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.12.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.12.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.12.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.12.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.12.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.12.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.12.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.12.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.13.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.13.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.13.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.13.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.13.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.13.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.13.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.13.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.13.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.14.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.14.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.14.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.14.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.14.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.14.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.14.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.14.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.14.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.15.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.15.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.15.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.15.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.15.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.15.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.15.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.15.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.15.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.16.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.16.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.16.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.16.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.16.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.16.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.16.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.16.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.16.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.17.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.17.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.17.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.17.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.17.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.17.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.17.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.17.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.17.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.18.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.18.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.18.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.18.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.18.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.18.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.18.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.18.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.18.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.19.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.19.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.19.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.19.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.19.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.19.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.19.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.19.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.19.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.20.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.20.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.20.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.20.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.20.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.20.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.20.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.20.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.20.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.21.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.21.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.21.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.21.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.21.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.21.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.21.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.21.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.21.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.22.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.22.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.22.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.22.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.22.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.22.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.22.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.22.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.22.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.23.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.23.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.23.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.23.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.23.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.23.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.23.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.23.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.23.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.24.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.24.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.24.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.24.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.24.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.24.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.24.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.24.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.24.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.25.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.25.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.25.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.25.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.25.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.25.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.25.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.25.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.25.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.26.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.26.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.26.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.26.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.26.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.26.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.26.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.26.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.26.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.27.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.27.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.27.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.27.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.27.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.27.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.27.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.27.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.27.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.28.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.28.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.28.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.28.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.28.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.28.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.28.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.28.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.28.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.29.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.29.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.29.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.29.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.29.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.29.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.29.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.29.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.29.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.30.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.30.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.30.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.30.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.30.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.30.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.30.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.30.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.30.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.31.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.31.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.31.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.31.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.31.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.31.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.31.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.31.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.31.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.norm.weight is on device: cuda:0\n",
      "lm_head.weight is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def clear_info_gpu():\n",
    "    \"\"\" Clear GPU cache and print info \"\"\"\n",
    "    # Clear the memory cache on the GPU\n",
    "    torch.cuda.empty_cache()\n",
    "    # Collect garbage to ensure all references are removed\n",
    "    gc.collect()\n",
    "    # Ensure all CUDA operations are finished before clearing memory (optional)\n",
    "    torch.cuda.synchronize()\n",
    "    # Print GPU info\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1024 ** 3\n",
    "    free_memory = total_memory - allocated_memory\n",
    "    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Allocated GPU memory: {allocated_memory:.2f} GB\")\n",
    "    print(f\"Free GPU memory: {free_memory:.2f} GB\")\n",
    "\n",
    "\n",
    "# Check GPU info\n",
    "clear_info_gpu()\n",
    "\n",
    "# Check the device of each module of the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is on device: {param.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV1-qVCnUWkf"
   },
   "source": [
    "# Inference\n",
    "\n",
    "Let's try the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VGx93sN0pAv9",
    "outputId": "c9e14a78-20ef-4f5d-9c5f-5ec68dfcf10a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi mate! I'm a 25-year-old Aussie bloke who loves playing guitar, writing songs, and singing my heart out. I've been playing music for\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate chat response\n",
    "def generate_response(prompt, max_new_tokens=30, temperature=0.2, num_beams=1):\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the output ids\n",
    "    outputs = model.generate(inputs.input_ids,\n",
    "                             attention_mask=inputs['attention_mask'],  # Avoid warning\n",
    "                             pad_token_id=tokenizer.eos_token_id,  # Avoid warning\n",
    "                             max_new_tokens=max_new_tokens,  # Length of generation\n",
    "                             temperature=temperature,  # Temperature for randomness\n",
    "                             num_beams=num_beams  # Number of beams\n",
    "                             )\n",
    "\n",
    "    # Decode the output ids to a string\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Try it\n",
    "generate_response(\"Hi mate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NBJlpV3X_pj"
   },
   "source": [
    "### Key Parameters in `model.generate()`\n",
    "\n",
    "There are numerous arguments for `model.generate()`. Below, we highlight the most useful ones.\n",
    "\n",
    "#### **Length Control**\n",
    "- **`max_length`**: Specifies the maximum number of tokens for the entire sequence, including both input tokens (from the prompt) and generated tokens. The model stops generating once the total number of tokens reaches this limit.\n",
    "- **`max_new_tokens`**: Defines the maximum number of new tokens that the model can generate, excluding the tokens from the input prompt. The model will generate up to this many tokens after receiving the input.\n",
    "- **`eos_token_id`**: The ID of the end-of-sequence (EOS) token. The generation will stop once the model generates this token, marking the end of the sequence.\n",
    "\n",
    "#### **Diversity and Quality**\n",
    "- **`temperature`**: Controls the randomness of predictions. Lower values (e.g., 0.7) make the model more deterministic, while higher values (e.g., 1.0 or above) increase randomness, making the outputs more diverse.\n",
    "- **`top_k`**: Limits the next token selection to the top `k` most likely tokens. A higher value allows for more variety in the generated text, while a lower value makes it more deterministic.\n",
    "- **`top_p` (nucleus sampling)**: Limits token selection to tokens with a cumulative probability of `p`. This ensures that only the top `p` percent of the probability mass is considered, promoting diverse but controlled generation.\n",
    "- **`do_sample`**: Enables random sampling of tokens instead of greedy decoding (which selects the highest-probability token). This is essential for generating diverse outputs.\n",
    "- **`num_beams`**: The number of beams for beam search. Higher values explore more possibilities during generation, leading to better outputs but at the cost of increased computation.\n",
    "\n",
    "![beam](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "#### **Repetition and Token Constraints**\n",
    "- **`repetition_penalty`**: Penalizes repeated tokens, discouraging the model from generating repetitive sequences. A value greater than 1.0 reduces the likelihood of repeating the same token.\n",
    "- **`no_repeat_ngram_size`**: Prevents repetition of n-grams of a specified size. For example, `no_repeat_ngram_size=3` ensures that trigrams do not repeat in the generated output.\n",
    "\n",
    "#### **Output Control**\n",
    "- **`num_return_sequences`**: The number of different sequences to generate. For example, `num_return_sequences=3` generates three separate outputs from the same prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Batch inference\n",
    "\n",
    "Now, we enhance our `generate_response()` function to support batch inference, which is crucial for serving multiple users in production environments. Additionally, we add functionality to measure the runtime of the inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VJ5Yd6M8Oj5o"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompts, max_new_tokens=30, temperature=0.2, num_beams=1, measure_time=False):\n",
    "    # Check if input is a single prompt or batch of prompts\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]  # Convert single prompt to a list for batch processing\n",
    "\n",
    "    # Tokenize the batch of prompts (single or multiple)\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "    # Start time measurement for model.generate() if requested\n",
    "    start_time = time.time() if measure_time else None\n",
    "\n",
    "    # Generate responses for the batch\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs['attention_mask'],  # Avoid warning\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Avoid warning\n",
    "        max_new_tokens=max_new_tokens,  # Length of generation\n",
    "        temperature=temperature,  # Temperature for randomness\n",
    "        num_beams=num_beams  # Number of beams\n",
    "    )\n",
    "\n",
    "    # Measure time after generation\n",
    "    if measure_time:\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "    else:\n",
    "        runtime = None\n",
    "\n",
    "    # Decode the batch of generated outputs\n",
    "    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    # If the original input was a single prompt, return a single string, not a list\n",
    "    if len(prompts) == 1:\n",
    "        responses = responses[0]\n",
    "\n",
    "    # Return both the response and runtime if time measurement was requested\n",
    "    return (responses, runtime) if measure_time else responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For batched inference, padding is necessary to ensure that all input sequences in a batch are of the same length. This allows the model to process multiple inputs in parallel. To set up padding correctly, we need to configure the tokenizer to handle padding. Specifically, we can assign a padding token (typically the `eos_token`) and set the padding side to ensure proper alignment of input sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the pad token to the eos token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's create a multi-input task, and measure its runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Once upon a time in a distant land... there was a magical kingdom hidden deep within the heart of a mystical forest. The kingdom was ruled by a wise and just king, who had a special\n",
      "---------------\n",
      "Explain the theory of relativity in simple terms. Albert Einstein's theory of relativity is a fundamental concept in modern physics that has been widely accepted and applied in various fields. In simple terms, the\n",
      "---------------\n",
      "What is the capital of France? Paris\n",
      "What is the capital of Germany? Berlin\n",
      "What is the capital of Italy? Rome\n",
      "What is the capital of Spain? Madrid\n",
      "What\n",
      "---------------\n",
      "Generate a story about a robot in the future. The robot is named Zeta and it is a highly advanced artificial intelligence that has been designed to assist humans in various tasks. Zeta is capable of\n",
      "---------------\n",
      "How do you bake a chocolate cake? Baking a chocolate cake is a simple process that requires a few basic ingredients and some basic cooking skills. Here's a step-by-step guide to help\n",
      "\n",
      "\n",
      "Runtime before acceleration: 2.345522880554199\n"
     ]
    }
   ],
   "source": [
    "# Batch input\n",
    "many_prompts = [\n",
    "    \"Once upon a time in a distant land...\",\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Generate a story about a robot in the future.\",\n",
    "    \"How do you bake a chocolate cake?\"\n",
    "]\n",
    "\n",
    "# Batch inference\n",
    "many_responses, t = generate_response(many_prompts, measure_time=True)\n",
    "\n",
    "# Print results\n",
    "for res in many_responses:\n",
    "    print(\"---------------\")\n",
    "    print(res)\n",
    "print(\"\\n\\nRuntime before acceleration:\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Acceleration\n",
    "\n",
    "In this section, we utilize **vLLM** to optimize inference through **paged attention**, as described in [this paper](https://arxiv.org/abs/2309.06180). The key idea of paged attention is to enhance memory efficiency by managing the key-value (KV) cache in a way that minimizes memory fragmentation. This is achieved by allocating memory in smaller \"pages\" instead of reserving large, contiguous memory blocks, which reduces memory fragmentation and allows for more flexible and efficient GPU memory usage. Through this approach, vLLM greatly improves inference throughput, particularly for batched or parallel requests, enabling Large Language Models (LLMs) to handle multiple sequences simultaneously while maintaining high performance on limited GPU resources.\n",
    "\n",
    "Before proceeding, we release the GPU memory by deleting the previous model, ensuring efficient usage of resources on a small GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 39.39 GB\n",
      "Allocated GPU memory: 0.99 GB\n",
      "Free GPU memory: 38.41 GB\n"
     ]
    }
   ],
   "source": [
    "# Delete the model and free its GPU memory\n",
    "try:\n",
    "    del model  # Deletes the model object\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "clear_info_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we wrap our Llama 3 model in the `vllm.LLM` wrapper. The `gpu_memory_utilization` parameter is essential, setting the proportion of GPU memory (between 0 and 1) allocated for model weights, activations, and the KV cache. Higher values increase the available KV cache size, enhancing throughput by allowing for more efficient processing of larger sequences or batches. However, setting this value too high risks out-of-memory (OOM) errors, so it’s crucial to balance utilization based on the specific memory capacity of the GPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-22 12:08:49 config.py:321] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 10-22 12:08:49 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=True, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-22 12:08:49 model_runner.py:1056] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n",
      "INFO 10-22 12:08:49 loader.py:1051] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 10-22 12:08:50 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db12e0b4fa8345848b7529f8d360bc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-22 12:08:53 model_runner.py:1067] Loading model weights took 5.3128 GB\n",
      "INFO 10-22 12:08:54 gpu_executor.py:122] # GPU blocks: 14541, # CPU blocks: 2048\n",
      "INFO 10-22 12:08:54 gpu_executor.py:126] Maximum concurrency for 8192 tokens per request: 28.40x\n",
      "INFO 10-22 12:08:55 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-22 12:08:55 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-22 12:09:13 model_runner.py:1523] Graph capturing finished in 18 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 39.39 GB\n",
      "Allocated GPU memory: 34.74 GB\n",
      "Free GPU memory: 4.65 GB\n"
     ]
    }
   ],
   "source": [
    "# Use vllm to load model\n",
    "model = vllm.LLM(model=model_name,\n",
    "                 skip_tokenizer_init=True,\n",
    "                 quantization=\"bitsandbytes\", load_format=\"bitsandbytes\",\n",
    "                 dtype=\"auto\", device=\"cuda\",\n",
    "                 gpu_memory_utilization=0.5,\n",
    "                 max_seq_len_to_capture=1024)\n",
    "\n",
    "# Check GPU info\n",
    "clear_info_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, modify the generate function for `vllm.LLM`. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_vllm(prompts, max_new_tokens=50, temperature=0.2, measure_time=False):\n",
    "    # Check if input is a single prompt or batch of prompts\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]  # Convert single prompt to a list for batch processing\n",
    "\n",
    "    # Tokenize the batch of prompts (single or multiple)\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    inputs = [vllm.TokensPrompt(prompt_token_ids=inp) for inp in inputs.data[\"input_ids\"].tolist()]\n",
    "\n",
    "    # Start time measurement for model.generate() if requested\n",
    "    start_time = time.time() if measure_time else None\n",
    "\n",
    "    # Generate responses for the batch\n",
    "    outputs = model.generate(\n",
    "        prompts=inputs,\n",
    "        sampling_params=vllm.SamplingParams(max_tokens=max_new_tokens, temperature=temperature),\n",
    "    )\n",
    "\n",
    "    # Measure time after generation\n",
    "    if measure_time:\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "    else:\n",
    "        runtime = None\n",
    "\n",
    "    # Decode the batch of generated outputs\n",
    "    responses = [tokenizer.decode(output.outputs[0].token_ids, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    # If the original input was a single prompt, return a single string, not a list\n",
    "    if len(prompts) == 1:\n",
    "        responses = responses[0]\n",
    "\n",
    "    # Return both the response and runtime if time measurement was requested\n",
    "    return (responses, runtime) if measure_time else responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-Fk2goXdObF-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-22 12:09:14 preprocess.py:63] Using None for EOS token id because tokenizer is not initialized\n",
      "WARNING 10-22 12:09:14 preprocess.py:63] Using None for EOS token id because tokenizer is not initialized\n",
      "WARNING 10-22 12:09:14 preprocess.py:63] Using None for EOS token id because tokenizer is not initialized\n",
      "WARNING 10-22 12:09:14 preprocess.py:63] Using None for EOS token id because tokenizer is not initialized\n",
      "WARNING 10-22 12:09:14 preprocess.py:63] Using None for EOS token id because tokenizer is not initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████| 5/5 [00:02<00:00,  1.99it/s, est. speed input: 23.87 toks/s, output: 60.47 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      " there was a small village nestled in the heart of a dense forest. The villagers lived simple lives, relying on the forest for their livelihood. They were a peaceful and harmonious community, living in tune with nature.\n",
      "One day, a young girl named\n",
      "---------------\n",
      " The theory of relativity, developed by Albert Einstein, is a fundamental concept in modern physics that has had a profound impact on our understanding of the universe. In simple terms, the theory of relativity states that the laws of physics are the same everywhere\n",
      "---------------\n",
      "\n",
      "---------------\n",
      " The story should be a mix of science fiction and fantasy. The story should also include a moral lesson.\n",
      "\n",
      "In the year 2154, in the city of New Eden, a robot named Zeta was created by the brilliant scientist, Dr. Rachel\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "Runtime before acceleration: 2.521496057510376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch inference\n",
    "many_responses, t = generate_response_vllm(many_prompts, measure_time=True)\n",
    "\n",
    "# Print results\n",
    "for res in many_responses:\n",
    "    print(\"---------------\")\n",
    "    print(res)\n",
    "print(\"\\n\\nRuntime before acceleration:\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zs-iF825Dvq"
   },
   "source": [
    "### Results\n",
    "\n",
    "The results indicate that while vLLM significantly increases GPU memory usage, it does not provide faster inference speeds in our specific setup.\n",
    "\n",
    "- **Increased memory usage**: This occurs because vLLM pre-allocates GPU memory to optimize execution. The pre-allocation is designed for handling large-scale workloads efficiently, which can lead to higher memory usage even when dealing with smaller models or batches.\n",
    "  \n",
    "- **No speedup observed**: The similar inference speeds are likely due to our small batch size and short sequence lengths. vLLM's paged attention is optimized for large batches and long sequences, where it can reduce latency by efficiently managing memory. For small workloads, the overhead of this optimization outweighs the potential speed gains.\n",
    "\n",
    "#### When to use vLLM\n",
    "\n",
    "You will benefit most from using vLLM if:\n",
    "\n",
    "- You have a large model (e.g., non-quantized models that require significant memory) and GPU (or GPU-cluster).\n",
    "- You are working with large batch sizes and/or long sequence lengths, where vLLM's paged attention can significantly improve speed and resource utilization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "In this section, we build a simple chatbot as our first application. \n",
    "\n",
    "First, let's re-create the model using `from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 39.39 GB\n",
      "Allocated GPU memory: 1.00 GB\n",
      "Free GPU memory: 38.40 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f2763c27e42eb988ea41df15b96bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Delete the model and free its GPU memory\n",
    "try:\n",
    "    del model  # Deletes the model object\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "clear_info_gpu()\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs do not retain memory between interactions. To illustrate this, consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the first three colors in the rainbow? The first three colors in the rainbow, in order, are: Red, Orange, and Yellow. The colors of the rainbow are often remembered using the\n",
      "-------\n",
      "What are the rest? (Part 2)\n",
      "In my previous post, I introduced the first 5 of the 7 rest positions in yoga. Here are the remaining \n"
     ]
    }
   ],
   "source": [
    "print(generate_response(\"What are the first three colors in the rainbow?\"))\n",
    "print(\"-------\")\n",
    "print(generate_response(\"What are the rest?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the model fails to interpret what \"the rest\" refers to. This is because LLMs process each query independently and do not retain the context of previous interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the limitation of LLMs not retaining memory between interactions, we can implement a **streamed history**. This technique involves maintaining a conversation history by appending previous user inputs and model responses to a single prompt. By doing so, we simulate context retention, allowing the model to handle follow-up questions more effectively.\n",
    "\n",
    "We will also provide a **world prompt** to set the stage for the conversation, leading the chatbot to respond in a specific context or theme. This world prompt acts as an overarching guide for the interaction, helping maintain consistency throughout the conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Rachel initialized. Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[You]  Hi Whats your name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rachel]  Hi there! My name is Rachel, nice to meet you! I'm here to help answer any questions you may have. What's on your mind?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[You]  What are the first three colors of rainbow?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rachel]  Ah, that's an easy one! The first three colors of the rainbow, in order, are Red, Orange, and Yellow. Would you like to know more about rainbows or is there something else I can help you with? \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[You]  How about the rest?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rachel]   The next three colors of the rainbow, in order, are Green, Blue, and Violet. So, the complete rainbow colors are: Red, Orange, Yellow, Green, Blue, and Violet. Would you like to know more about the science behind rainbows or is there something else I can help you with? \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[You]  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending the conversation.\n"
     ]
    }
   ],
   "source": [
    "# Give your bot a name\n",
    "bot_name = \"Rachel\"\n",
    "\n",
    "# Initial world prompt to guide the conversation\n",
    "world_prompt = f\"\"\"\n",
    "Your name is {bot_name}. \n",
    "You are a knowledgeable assistant, capable of answering questions on a wide variety of topics. \n",
    "Assist the user with information, advice, or explanations based on their queries. \n",
    "You only generate your own answer. Do not continue to generate user input.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the conversation history with the world prompt\n",
    "history = world_prompt\n",
    "\n",
    "print(f\"Chatbot {bot_name} initialized. Type 'exit' to end the conversation.\\n\")\n",
    "\n",
    "# Loop for continuous user input\n",
    "while True:\n",
    "    # Get user input\n",
    "    user_input = input(\"[You] \")\n",
    "\n",
    "    # Exit condition\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Ending the conversation.\")\n",
    "        break\n",
    "\n",
    "    # Append the new user input to the history\n",
    "    final_prompts = f\"{history}\\nUser: {user_input}\\n{bot_name}:\"\n",
    "\n",
    "    # Generate chatbot response using the existing generate_response function\n",
    "    response = generate_response(final_prompts, max_new_tokens=100)\n",
    "\n",
    "    # Exclude the input tokens (prompt) from the response by slicing out the history part\n",
    "    generated_text = response[len(final_prompts):]  # Only take the newly generated part\n",
    "\n",
    "    # Only take the first line because further conversation may be generated\n",
    "    if \"\\n\" in generated_text:\n",
    "        generated_text = generated_text.split(\"\\n\")[0]\n",
    "\n",
    "    # Print the chatbot response without including the prompt\n",
    "    print(f\"[{bot_name}] {generated_text}\\n\")\n",
    "\n",
    "    # Update conversation history with the user input and chatbot response\n",
    "    history = f\"{history}\\nUser: {user_input}\\n{bot_name}: {generated_text}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our chatbot works as expected. However, as the conversation continues, the history accumulates, leading to **an increasingly larger sequence length**. This will gradually slow down inference and, eventually, hit the token limit of the model. To address this, we can apply advanced prompt engineering techniques, such as conversation chaining (e.g., in LangChain), which helps manage context by retaining only the most relevant parts of the conversation while discarding older, less relevant exchanges. This keeps the sequence length manageable and ensures efficient inference over time."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More Exercises\n",
    "\n",
    "* Experiment with different models available on Hugging Face to compare their performance and suitability for various tasks.\n",
    "* Explore additional parameters in `model.generate()`, such as `temperature`, `top_k`, and `num_beams`, to understand their impact on the quality and creativity of responses, as well as the inference speed.\n",
    "* Enhance the chatbot's memory management: when the chat history exceeds a certain length, prompt the LLM to summarize the conversation into a concise paragraph. This will help maintain context while reducing token usage and improving efficiency. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d6eb16f5f5a4a5ca7d2f2caa21d4b5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13d2c8aca673481184654e1af26a2b73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2bb3efd6d1794a01b66c27b867409c2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93a79789eaf4404cbb1e66680275d347",
       "IPY_MODEL_f4a1e41696564260af9edf4c87f120ba",
       "IPY_MODEL_ab99f154499641c7b16a724d09409956"
      ],
      "layout": "IPY_MODEL_4f8eae8335f147bbab1107f619f30d24"
     }
    },
    "3f43a26e911f4f46b9802186942fe408": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f8eae8335f147bbab1107f619f30d24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "801549846c88431792724812adc8e408": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89c6f44f7f2c4adca32af7d6f3a58452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93a79789eaf4404cbb1e66680275d347": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f43a26e911f4f46b9802186942fe408",
      "placeholder": "​",
      "style": "IPY_MODEL_89c6f44f7f2c4adca32af7d6f3a58452",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "ab99f154499641c7b16a724d09409956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d6eb16f5f5a4a5ca7d2f2caa21d4b5d",
      "placeholder": "​",
      "style": "IPY_MODEL_801549846c88431792724812adc8e408",
      "value": " 2/2 [00:47&lt;00:00, 23.18s/it]"
     }
    },
    "f4a1e41696564260af9edf4c87f120ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5cb7c7eb6ef4069bd13099582cf5453",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13d2c8aca673481184654e1af26a2b73",
      "value": 2
     }
    },
    "f5cb7c7eb6ef4069bd13099582cf5453": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
