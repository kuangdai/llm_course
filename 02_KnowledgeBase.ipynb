{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324a765292b97b43",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we explore how to create structured knowledge bases from a dataset. These offline processes are designed to greatly enhance online retrieval-augmented generation (RAG) applications.\n",
    "\n",
    "We begin with the **Poetry Foundation** dataset, using a large language model (LLM) to generate embeddings from the last hidden states. These embeddings capture the semantic meanings of the poems and are managed by a **FAISS** index, enabling efficient similarity search.\n",
    "\n",
    "We then leverage the LLM to infer keywords from each poem, identifying key concepts within the text. These keywords are used to construct a knowledge graph with **NetworkX**, providing a structured representation of relationships between poems and keywords.\n",
    "\n",
    "By building both a vector database and a knowledge graph, this notebook reveals the thematic connections and structures within the poetry dataset. In subsequent sessions, we will integrate these knowledge bases into our multi-agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35001e9a1323e57",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import faiss\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb730c8a9f17c0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Inspecting Data\n",
    "\n",
    "In this notebook, we will use the [Poetry Foundation Poems](https://www.kaggle.com/datasets/tgdivy/poetry-foundation-poems/code). It contains nearly all poems from the [Poetry Foundation Website](https://www.poetryfoundation.org/). \n",
    "\n",
    "### Why Poetry?\n",
    "```\n",
    "In beauty's grasp, the verses play,\n",
    "Concise in form, they light the way.\n",
    "Ideal for big thoughts on small devices,\n",
    "In every line, a world that entices.\n",
    "```\n",
    "\n",
    "Let's begin by examining the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874904524cf4a40c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the .npz file with allow_pickle=True\n",
    "loaded_npz = np.load('data/input/poetry_data_clean.npz', allow_pickle=True)\n",
    "\n",
    "# Reconstruct the DataFrame using the saved data and columns\n",
    "df = pd.DataFrame(loaded_npz['df'], columns=loaded_npz['columns'])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734057cfe5ca2c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def format_poems(idx, include_tags=False):\n",
    "    \"\"\"Format poems\"\"\"\n",
    "    if hasattr(idx, \"__len__\"):\n",
    "        return [format_poems(i_, include_tags) for i_ in idx]\n",
    "    it = df.iloc[idx]\n",
    "    res = f'{it[\"Title\"]}\\n{it[\"Poet\"]}\\n\\n{it[\"Poem\"]}'\n",
    "    if it[\"Tags\"] and include_tags:\n",
    "        res += f'\\n\\nNotes: {it[\"Tags\"]}'\n",
    "    return res\n",
    "\n",
    "\n",
    "print(format_poems(99, include_tags=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b5ed099e8e10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Vector Database\n",
    "\n",
    "In this section, we will prepare a vector database using **FAISS**, a library designed for efficient similarity search and clustering of dense vectors. \n",
    "\n",
    "### Deep Embeddings\n",
    "To leverage fast and scalable vector-based algorithms for retrieval-augmented generation (RAG), the data must first be encoded into real-valued vectors. This can be achieved by passing the text through an LLM and using the output from the last hidden layer as the embeddings. \n",
    "\n",
    "**Why use the whole transformer?** \n",
    "Using the entire transformer allows us to capture the contextual information that may be critical for understanding the semantics of the text. The **deep embeddings** generated from the last hidden layer are enriched with this context, making them more effective for similarity searches.\n",
    "\n",
    "First, let's load our tokenizer and model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b5dfd4bfb64d8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read HF_ACCESS_KEY into hf_access_key\n",
    "with open(\"api_keys.json\", \"r\") as file:\n",
    "    hf_access_key = json.load(file).get(\"HF_ACCESS_KEY\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(hf_access_key)\n",
    "\n",
    "# Create a BitsAndBytesConfig for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Change this to `False` to disable quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Optional for performance\n",
    "    bnb_4bit_quant_type='nf4',  # Normal floating-point 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Set compute dtype to float16 for faster inference\n",
    ")\n",
    "\n",
    "# Model name--you can change to many huggingface models\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b50f1-b53e-4d67-af99-cdb18cbcf4ff",
   "metadata": {},
   "source": [
    "Next, we compute the deep embeddings. \n",
    "\n",
    "Embedding computation for the entire dataset may take up to **an hour on an A100 GPU**.\n",
    "To save time or if you don't have access to a powerful GPU, we provide a pre-generated vector database that allows you to skip the embedding process entirely using the following flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcc19f-6c38-4312-923d-ed5d6b810c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_MASSIVE_COMPUTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669a2851d62df61",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not SKIP_MASSIVE_COMPUTATION:\n",
    "    # List to store the embeddings for each text\n",
    "    embeddings_list = []\n",
    "\n",
    "    # Loop through the texts and display a progress bar using tqdm.notebook.trange\n",
    "    for i in trange(len(df), desc=\"Embedding Texts\"):\n",
    "        # Tokenize the individual text (convert text to token IDs, apply padding)\n",
    "        inputs = tokenizer([format_poems(i)], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Pass input through the model with output_hidden_states=True\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        # Extract the hidden states from the output (last hidden state layer)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Shape: [batch_size, seq_length, hidden_size]\n",
    "\n",
    "        # Get the attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1).expand(\n",
    "            hidden_states.size())  # Shape: [batch_size, seq_length, hidden_size]\n",
    "\n",
    "        # Apply the mask to zero out the padding token embeddings\n",
    "        masked_hidden_states = hidden_states * attention_mask  # Shape: [batch_size, seq_length, hidden_size]\n",
    "\n",
    "        # Compute the sum of the embeddings for non-padding tokens\n",
    "        sum_embeddings = masked_hidden_states.sum(dim=1)  # Sum over the sequence dimension\n",
    "\n",
    "        # Compute the number of non-padding tokens for each sentence\n",
    "        non_pad_tokens = attention_mask.sum(dim=1)  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        # Perform mean pooling by dividing the sum by the number of non-padding tokens\n",
    "        embedding = sum_embeddings / non_pad_tokens.clamp(min=1e-9)  # Avoid division by zero\n",
    "\n",
    "        # Append the embedding to the list\n",
    "        embeddings_list.append(embedding)\n",
    "\n",
    "    # Concatenate all the embeddings into a single tensor\n",
    "    deep_embeddings = torch.cat(embeddings_list, dim=0).cpu()\n",
    "\n",
    "    # Save results\n",
    "    torch.save(deep_embeddings, 'data/knowledge_bases/poetry_embeddings.pt')\n",
    "\n",
    "else:\n",
    "    # Download pre-generated\n",
    "    # Google Drive file link and output path\n",
    "    google_drive_link = \"https://drive.google.com/uc?id=18z1pVoJbl66gB7HdkXxa7-UOzi99y764\"  # noqa\n",
    "    output_path = 'data/knowledge_bases/poetry_embeddings.pt'\n",
    "\n",
    "    # Check if the file already exists before downloading\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"File not found locally. Downloading from Google Drive to {output_path}...\")\n",
    "        gdown.download(google_drive_link, output_path, quiet=False)\n",
    "    else:\n",
    "        print(f\"File already exists at {output_path}. Skipping download.\")\n",
    "\n",
    "    # Load the embeddings after download or if already present\n",
    "    deep_embeddings = torch.load(output_path, weights_only=True)\n",
    "\n",
    "# Change to numpy\n",
    "deep_embeddings = deep_embeddings.numpy()\n",
    "print(\"Embedding shape:\", deep_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b6e3f-7547-4f25-be7f-e22842705bae",
   "metadata": {},
   "source": [
    "### FAISS Index\n",
    "\n",
    "Now we create the FAISS index. There are many different methods we can use, and typical ones are listed below:\n",
    "\n",
    "\n",
    "| Name          | Speed (1-5) | Memory (1-5) | Accuracy (1-5) | ANN* (Yes/No) | Description                                                                      |\n",
    "|---------------|-------------|--------------|----------------|---------------|----------------------------------------------------------------------------------|\n",
    "| IndexFlatL2   | 2           | 5            | 5              | No            | Direct computation of distances; exact nearest neighbor search.                  |\n",
    "| IndexIVFFlat  | 4           | 3            | 3              | Yes           | Uses inverted file indexing to search within clusters, improving speed.          |\n",
    "| IndexIVFPQ    | 5           | 2            | 3              | Yes           | Combines IVF with product quantization for fast and memory-efficient search.     |\n",
    "| IndexHNSWFlat | 5           | 4            | 4              | Yes           | Utilizes a hierarchical navigable small world graph for efficient searching.     |\n",
    "| IndexLSH      | 5           | 2            | 2              | Yes           | Employs locality-sensitive hashing for fast approximate nearest neighbor search. |\n",
    "\n",
    "* **ANN** stands for **Approximate Nearest Neighbor**. This refers to algorithms that find points in a high-dimensional space that are close to a given query point but do not guarantee exact matches. ANN methods trade off some accuracy for improved speed and efficiency, making them suitable for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95786d8-8d76-493a-8d72-000ede05f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HNSW FAISS index\n",
    "embedding_dim = deep_embeddings.shape[1]  # Dimension of your embeddings\n",
    "num_neighbors = 32  # Number of connections (neighbors) for each node\n",
    "index = faiss.IndexHNSWFlat(embedding_dim, num_neighbors)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(deep_embeddings)\n",
    "\n",
    "# Save index to file\n",
    "faiss.write_index(index, \"data/knowledge_bases/poetry_faiss.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae9f74-ea94-4ab1-9223-6f830728de24",
   "metadata": {},
   "source": [
    "Perform a search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a0f2c-dc82-4e8c-a0cb-67e76fb82f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the nearest neighbors of the first embedding\n",
    "query_embedding = deep_embeddings[0:1]\n",
    "\n",
    "# k is number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_embedding, k=5)\n",
    "\n",
    "# Step 4: Display the results\n",
    "print(\"Nearest neighbors (indices):\", indices)\n",
    "print(\"Distances to the nearest neighbors:\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a3bce-c311-4f9b-a8d8-07a5343f6fb2",
   "metadata": {},
   "source": [
    "# Knowledge Graph\n",
    "\n",
    "In addition to the FAISS index, we will also create a keyword-based knowledge graph using **NetworkX**, a powerful graph database management system.\n",
    " \n",
    "First, we identify up to ten keywords for each poem to serve as \"nodes\" in a bipartite graph, where keyword nodes connect to corresponding poem nodes. This knowledge graph enables rapid keyword-based retrieval and supports **associative retrieval**, broadening the scope of search possibilities.\n",
    "\n",
    "\n",
    "### Keyword Identification\n",
    "\n",
    "We will utilize our LLM to identify keywords through natural language prompts. Additionally, traditional NLP methods such as TF-IDF and RAKE can be employed for this task. While these methods are generally more efficient, they may lack the deeper comprehension needed to uncover nuanced themes, especially those in poems.\n",
    "\n",
    "**Prompts** are crucial for the quality of LLM-based reasoning. In this example, our prompt provides clear instructions along with an example to outline the **goal, requirements, and output format**. For example, the presence of the `[` character at the end is key to achieving a stable format, leveraging the autoregressive nature of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2ffe4-690d-4c3f-a54b-413edfb7bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt_template = (\"Identify or infer up to 10 semantically meaningful keywords from the following poem. \" +\n",
    "                   \"The keywords should be commonly used nouns or verbs. \" +\n",
    "                   \"Provide the keywords directly after `YOUR ANSWER:`, formatted within brackets and separated by commas, such as \" +\n",
    "                   \"YOUR ANSWER: [teacher, classroom].\\n\" +\n",
    "                   \"\\n\\n%s\\n\\n%s\\n\\n\\n\" +\n",
    "                   \"YOUR ANSWER: [\")\n",
    "\n",
    "# A prompt example\n",
    "print(prompt_template % (\"POEM TITLE\", \"POEM BODY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8df22-5c18-4919-95b3-8902182092f3",
   "metadata": {},
   "source": [
    "Again, this process is computationally expensive and may take up to **five hours on an A100 GPU**. Therefore, we provide a pre-generated version available for download (already in repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db8092-7e5c-484e-ac7e-f15e02daf8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_MASSIVE_COMPUTATION:\n",
    "    # List to store the keywords for each text\n",
    "    keywords_list = []\n",
    "\n",
    "    # Loop through the texts and display a progress bar using tqdm.notebook.trange\n",
    "    pbar = trange(len(df), desc=\"Extracting Keywords\")\n",
    "    for i in pbar:\n",
    "        # Create a prompt\n",
    "        prompt = prompt_template % (df.iloc[i]['Title'], df.iloc[i]['Poem'])\n",
    "\n",
    "        # Encode the prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate output\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs['input_ids'].cuda(),\n",
    "                                     max_new_tokens=25, temperature=0.1,\n",
    "                                     attention_mask=inputs['attention_mask'],\n",
    "                                     pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        # Contents after \"ANSWER:\"\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"YOUR ANSWER:\")[-1].strip()\n",
    "\n",
    "        # Parsing results\n",
    "        first_left_index = generated_text.find(\"[\")\n",
    "        first_right_index = generated_text.find(\"]\")\n",
    "        keywords_text = generated_text[first_left_index + 1:first_right_index]\n",
    "        keywords = [keyword.strip() for keyword in keywords_text.split(\",\") if keyword.strip()]\n",
    "\n",
    "        # Store in results\n",
    "        keywords_list.append(keywords)\n",
    "        pbar.set_postfix(keywords=str(keywords))\n",
    "\n",
    "    # Save keywords_list to a pickle file\n",
    "    with open('data/knowledge_bases/poetry_keywords.pkl', 'wb') as f:\n",
    "        pickle.dump(keywords_list, f)\n",
    "else:\n",
    "    # Load the keywords after download or if already present\n",
    "    with open('data/knowledge_bases/poetry_keywords.pkl', 'rb') as f:\n",
    "        keywords_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0156e-b991-4c50-b928-e121c448f0c1",
   "metadata": {},
   "source": [
    "Next, we will parse the results using **spaCy**, a powerful tool for performing fine-scale NLP tasks.\n",
    "\n",
    "1. Retain only the nouns and verbs.\n",
    "2. Convert them to their lemmatized form (e.g., `eggs` to `egg`, and `ran` to `run`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93f71d-98c0-4d38-9695-50dd79fea89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# List to store the cleaned keywords\n",
    "cleaned_keywords_list = []\n",
    "\n",
    "# Process each list of keywords individually\n",
    "for keywords in tqdm(keywords_list, \"Parsing keywords\"):\n",
    "    filtered_keywords = []\n",
    "    for keyword in keywords:\n",
    "        # Process each keyword individually\n",
    "        doc = nlp(keyword)\n",
    "        if doc[0].pos_ in ['NOUN', 'PROPN', 'VERB']:  # filter\n",
    "            filtered_keywords.append(doc[0].lemma_)  # lemmatize\n",
    "\n",
    "    # Append the cleaned keywords to the list\n",
    "    cleaned_keywords_list.append(filtered_keywords)\n",
    "\n",
    "# Output the cleaned keywords\n",
    "for i in range(5):\n",
    "    print(\"Before:\", keywords_list[i])\n",
    "    print(\"After: \", cleaned_keywords_list[i])\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf3fd1-837d-4b1a-900d-bd8d048257df",
   "metadata": {},
   "source": [
    "Next, we will determine a set of unique keywords and establish the forward and inverse mappings:\n",
    "\n",
    "* **Forward Mapping**: Maps poems to their corresponding keywords.\n",
    "* **Inverse Mapping**: Maps keywords to the poems in which they appear.\n",
    "\n",
    "These mappings can facilitate our future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c0b7a-ff90-4e66-a975-58ce3e129793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the cleaned keywords and get unique keywords\n",
    "flattened_keywords = [kw for sublist in cleaned_keywords_list for kw in sublist]\n",
    "unique_keywords_list = list(set(flattened_keywords))\n",
    "\n",
    "# Create a mapping from keyword to its index for O(1) lookups\n",
    "keyword_to_index = {kw: idx for idx, kw in enumerate(unique_keywords_list)}\n",
    "\n",
    "# Initialize inverse mapping\n",
    "inverse_mapping = [[] for _ in range(len(unique_keywords_list))]\n",
    "\n",
    "# Initialize forward mapping with the same structure as inverse_mapping\n",
    "forward_mapping = [[] for _ in range(len(cleaned_keywords_list))]\n",
    "\n",
    "# Create forward and inverse mappings in one loop\n",
    "for poem_index, keywords in enumerate(cleaned_keywords_list):\n",
    "    for kw in keywords:\n",
    "        if kw in keyword_to_index:  # Check if the keyword exists in the mapping\n",
    "            keyword_index = keyword_to_index[kw]\n",
    "            forward_mapping[poem_index].append(keyword_index)  # Pop forward mapping\n",
    "            inverse_mapping[keyword_index].append(poem_index)  # Pop inverse mapping\n",
    "\n",
    "# Output the results\n",
    "print(\"Number of Unique Keywords:\", len(unique_keywords_list))\n",
    "print(\"Unique Keywords List:\", unique_keywords_list[:3])\n",
    "print(\"Forward Mapping:\", forward_mapping[:3])\n",
    "print(\"Inverse Mapping:\", inverse_mapping[:3])\n",
    "\n",
    "# Save the results\n",
    "with open('data/knowledge_bases/poetry_unique_keywords.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_keywords_list, f)\n",
    "with open('data/knowledge_bases/poetry_forward_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(forward_mapping, f)\n",
    "with open('data/knowledge_bases/poetry_inverse_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(inverse_mapping, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee0990-075a-4b79-97ac-fbdef7da1ea0",
   "metadata": {},
   "source": [
    "### Graph Creation\n",
    "\n",
    "We can now create a **bipartite graph** using NetworkX, consisting of two types of nodes:\n",
    "\n",
    "* **Poem Nodes**: Each node represents a poem.\n",
    "* **Keyword Nodes**: Each node represents a keyword.\n",
    "\n",
    "An edge connects a poem node to a keyword node if the keyword is one of the poem’s associated keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6a90a-a5fc-477d-868a-7bef5e9d4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph (pk means poem-keyword bipartite graph)\n",
    "pk_graph = nx.Graph()\n",
    "\n",
    "# Add nodes for each unique keyword\n",
    "pk_graph.add_nodes_from(unique_keywords_list)\n",
    "\n",
    "# Add nodes for each poem\n",
    "num_poems = len(forward_mapping)\n",
    "pk_graph.add_nodes_from(list(range(num_poems)))\n",
    "\n",
    "# Add edges\n",
    "for i in range(num_poems):\n",
    "    for j in forward_mapping[i]:\n",
    "        pk_graph.add_edge(i, unique_keywords_list[j])\n",
    "\n",
    "# Save the graph\n",
    "with open(\"data/knowledge_bases/poetry_keyword_graph.gpickle\", \"wb\") as f:\n",
    "    pickle.dump(pk_graph, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca8709-8d0f-49fc-90dc-532569dfb17f",
   "metadata": {},
   "source": [
    "Let's visualize the top K nodes of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b28413-94ff-42f9-ac7f-6f40945a4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many keyword nodes to visualize\n",
    "n_keyword_nodes = 20\n",
    "\n",
    "# Compute degree centrality of graph\n",
    "degree_centrality = nx.degree_centrality(pk_graph)\n",
    "\n",
    "# Sort keyword nodes by centrality and take the top K\n",
    "top_keyword_nodes = [node for node, _ in\n",
    "                     sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:n_keyword_nodes]]\n",
    "\n",
    "# Track connections of poem nodes to keyword nodes\n",
    "poem_connections = Counter()\n",
    "\n",
    "# Find all poem nodes connected to these top keywords\n",
    "for kw_node in top_keyword_nodes:\n",
    "    for poem_node in pk_graph.neighbors(kw_node):\n",
    "        poem_connections[poem_node] += 1\n",
    "\n",
    "# Select poem nodes that connect to at least two of the top keyword nodes\n",
    "connected_poem_nodes = {node for node, count in poem_connections.items() if count >= 6}\n",
    "\n",
    "# Create a subgraph with only the top nodes and their edges\n",
    "subgraph = pk_graph.subgraph(top_keyword_nodes + list(connected_poem_nodes))\n",
    "\n",
    "# Create a layout for the subgraph\n",
    "pos = nx.spring_layout(subgraph)\n",
    "\n",
    "# Draw the subgraph with different styles for keyword and poem nodes\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Draw keyword nodes in a different color and shape (e.g., circles)\n",
    "nx.draw_networkx_nodes(subgraph, pos, nodelist=[node for node in top_keyword_nodes if node in subgraph],\n",
    "                       node_size=800, node_color='skyblue', node_shape='o', alpha=0.8, label=\"Keywords\")\n",
    "\n",
    "# Draw poem nodes in a different color and shape (e.g., squares)\n",
    "nx.draw_networkx_nodes(subgraph, pos, nodelist=[node for node in connected_poem_nodes if node in subgraph],\n",
    "                       node_size=800, node_color='salmon', node_shape='s', alpha=0.8, label=\"Poems\")\n",
    "\n",
    "# Draw edges and labels\n",
    "nx.draw_networkx_edges(subgraph, pos, width=1, alpha=0.5, edge_color='gray')\n",
    "nx.draw_networkx_labels(subgraph, pos, font_size=10)\n",
    "\n",
    "# Add title and legend\n",
    "plt.title(f\"Top {n_keyword_nodes} Nodes by Degree Centrality\")\n",
    "plt.legend(scatterpoints=1, labelspacing=3, frameon=False)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da849a-23e3-4610-87cb-81bf8ab8d6e3",
   "metadata": {},
   "source": [
    "Finally, let's perform some tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d611b8-d396-4966-9d84-288f0867caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the Number of Steps Between a Poem and a Keyword\n",
    "keyword = \"sunset\"\n",
    "poem_node = 5  # Suppose poem index 5 is a specific poem node\n",
    "try:\n",
    "    distance = nx.shortest_path_length(pk_graph, source=keyword, target=poem_node)\n",
    "    print(f\"\\nThe distance between '{keyword}' and poem {poem_node} is {distance} steps.\")\n",
    "except nx.NetworkXNoPath:\n",
    "    print(f\"\\nNo path found between '{keyword}' and poem {poem_node}.\")\n",
    "\n",
    "# Getting All Keywords within a Certain Path Depth from a Poem\n",
    "poem_node = 3  # Suppose poem index 3 is a specific poem\n",
    "path_depth = 2\n",
    "related_keywords = nx.single_source_shortest_path_length(pk_graph, poem_node, cutoff=path_depth)\n",
    "\n",
    "# Filter out only keyword nodes\n",
    "related_keywords = {node: dist for node, dist in related_keywords.items() if node in unique_keywords_list}\n",
    "\n",
    "print(f\"\\nKeywords within {path_depth} steps of poem {poem_node}: {related_keywords}\")\n",
    "\n",
    "# Finding the Degree of Centrality for Each Keyword Node\n",
    "centrality = nx.degree_centrality(pk_graph)\n",
    "top_keywords = sorted([(node, cent) for node, cent in centrality.items() if node in unique_keywords_list],\n",
    "                      key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop keywords by centrality:\")\n",
    "for keyword, score in top_keywords[:10]:  # Display the top 10 keywords\n",
    "    print(f\"Keyword: {keyword}, Centrality Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ee66c99d4c81e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# More Exercises\n",
    "\n",
    "- **Utilize Shallow Embedding**: Build a vector database using **shallow embeddings** derived from the output of the embedding layer (e.g., `model.base_model.embed_tokens` for Llama 3). Shallow embeddings are faster to compute as they involve only a single layer. Reflect on why deep embeddings, despite requiring more computation, may still be preferred in some applications.\n",
    "\n",
    "- **Explore FAISS Index Types**: Investigate the various index types available in FAISS, examining their trade-offs in terms of speed, memory usage, and accuracy. Evaluate which index types may be best suited for different retrieval scenarios.\n",
    "\n",
    "- **Implement Traditional NLP Methods for Keyword Extraction**: Apply traditional NLP methods, such as TF-IDF and RAKE, to extract keywords. Compare these results with those generated by the LLM to understand the strengths and limitations of each approach in terms of effectiveness and efficiency.\n",
    "\n",
    "- **Add Edges Between Poems Based on Deep Embedding Similarity**: Consider adding edges between poems that are similar based on their deep embeddings. This could enhance retrieval by connecting works with thematic or stylistic similarities.\n",
    "\n",
    "- **Add Poet Names as Nodes**: Introduce poet names as additional nodes, connecting each poet to their respective works. Explore how this setup could enable retrieval based on authorship and potentially similar styles or themes among poets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b00e425f22d47",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
