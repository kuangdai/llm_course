{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324a765292b97b43",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we explore how to create structured knowledge bases from a dataset. These offline processes are designed to greatly enhance online retrieval-augmented generation (RAG) applications.\n",
    "\n",
    "We begin with the **Poetry Foundation** dataset, using a large language model (LLM) to generate embeddings from the last hidden states. These embeddings capture the semantic meanings of the poems and are managed by a **FAISS** index, enabling efficient similarity search.\n",
    "\n",
    "We then leverage the LLM to infer keywords from each poem, identifying key concepts within the text. These keywords are used to construct a knowledge graph with **NetworkX**, providing a structured representation of relationships between keywords based on co-occurrence and semantic similarity.\n",
    "\n",
    "By building both a vector database and a keyword graph, this notebook reveals the thematic connections and structures within the poetry dataset. In subsequent sessions, we will integrate these knowledge bases into our multi-agent system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35001e9a1323e57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T21:28:40.638628Z",
     "start_time": "2024-10-23T21:28:38.873210Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import faiss\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb730c8a9f17c0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Inspect data\n",
    "\n",
    "In this notebook, we will use the [Poetry Foundation Poems](https://www.kaggle.com/datasets/tgdivy/poetry-foundation-poems/code). It contains nearly all poems from the [Poetry Foundation Website](https://www.poetryfoundation.org/). \n",
    "\n",
    "### Why Poetry?\n",
    "```\n",
    "In beauty's grasp, the verses play,\n",
    "Concise in form, they light the way.\n",
    "Ideal for big thoughts on small devices,\n",
    "In every line, a world that entices.\n",
    "```\n",
    "\n",
    "Let's begin by examining the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874904524cf4a40c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T21:55:46.521854Z",
     "start_time": "2024-10-23T21:55:46.093359Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the .npz file with allow_pickle=True\n",
    "loaded_npz = np.load('data/poetry_data_clean.npz', allow_pickle=True)\n",
    "\n",
    "# Reconstruct the DataFrame using the saved data and columns\n",
    "df = pd.DataFrame(loaded_npz['df'], columns=loaded_npz['columns'])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(len(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734057cfe5ca2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T21:28:40.817795Z",
     "start_time": "2024-10-23T21:28:40.815920Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatted(idx):\n",
    "    \"\"\" format poems \"\"\"\n",
    "    it = df.iloc[idx]\n",
    "    res = f'{it[\"Title\"]}\\n{it[\"Poet\"]}\\n--------\\n\\n{it[\"Poem\"]}'\n",
    "    if it[\"Tags\"]:\n",
    "        res += f'\\n\\n--------\\nNotes: {it[\"Tags\"]}'\n",
    "    return res\n",
    "\n",
    "\n",
    "print(formatted(99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b5ed099e8e10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Vector Database\n",
    "\n",
    "In this section, we will prepare a vector database using **FAISS**, a library designed for efficient similarity search and clustering of dense vectors. \n",
    "\n",
    "### Deep embeddings\n",
    "To leverage fast and scalable vector-based algorithms for retrieval-augmented generation (RAG), the data must first be encoded into real-valued vectors. This can be achieved by passing the text through an LLM and using the output from the last hidden layer as the embeddings. \n",
    "\n",
    "**Why use the whole transformer?** \n",
    "Using the entire transformer allows us to capture the contextual information that may be critical for understanding the semantics of the text. The **deep embeddings** generated from the last hidden layer are enriched with this context, making them more effective for similarity searches.\n",
    "\n",
    "First, let's load our tokenizer and model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b5dfd4bfb64d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T21:28:40.988459Z",
     "start_time": "2024-10-23T21:28:40.818736Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Login Huggingface\n",
    "hf_access_key = \"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\"  # noqa\n",
    "login(hf_access_key)\n",
    "\n",
    "# Create a BitsAndBytesConfig for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Change this to `False` to disable quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Optional for performance\n",
    "    bnb_4bit_quant_type='nf4',  # Normal floating-point 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Set compute dtype to float16 for faster inference\n",
    ")\n",
    "\n",
    "# Model name--you can change to many huggingface models\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b50f1-b53e-4d67-af99-cdb18cbcf4ff",
   "metadata": {},
   "source": [
    "Next, we compute the deep embeddings. \n",
    "\n",
    "Embedding computation for the entire dataset may take up to **an hour on an A100 GPU**.\n",
    "To save time or if you don't have access to a powerful GPU, we provide a pre-generated vector database that allows you to skip the embedding process entirely using the following flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcc19f-6c38-4312-923d-ed5d6b810c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_MASSIVE_COMPUTATION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669a2851d62df61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T21:28:42.111573Z",
     "start_time": "2024-10-23T21:28:42.108502Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not SKIP_MASSIVE_COMPUTATION:\n",
    "    # List to store the embeddings for each text\n",
    "    embeddings_list = []\n",
    "\n",
    "    # Loop through the texts and display a progress bar using tqdm.notebook.trange\n",
    "    for i in trange(len(df), desc=\"Embedding Texts\"):\n",
    "        # Tokenize the individual text (convert text to token IDs, apply padding)\n",
    "        inputs = tokenizer([formatted(i)], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        # Pass input through the model with output_hidden_states=True\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        # Extract the hidden states from the output (last hidden state layer)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Shape: [batch_size, seq_length, hidden_size]\n",
    "\n",
    "        # Get the attention mask (1 for real tokens, 0 for padding)\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1).expand(\n",
    "            hidden_states.size())  # Shape: [batch_size, seq_length, hidden_size]\n",
    "\n",
    "        # Apply the mask to zero out the padding token embeddings\n",
    "        masked_hidden_states = hidden_states * attention_mask  # Shape: [batch_size, seq_length, hidden_size]\n",
    "\n",
    "        # Compute the sum of the embeddings for non-padding tokens\n",
    "        sum_embeddings = masked_hidden_states.sum(dim=1)  # Sum over the sequence dimension\n",
    "\n",
    "        # Compute the number of non-padding tokens for each sentence\n",
    "        non_pad_tokens = attention_mask.sum(dim=1)  # Shape: [batch_size, hidden_size]\n",
    "\n",
    "        # Perform mean pooling by dividing the sum by the number of non-padding tokens\n",
    "        embedding = sum_embeddings / non_pad_tokens.clamp(min=1e-9)  # Avoid division by zero\n",
    "\n",
    "        # Append the embedding to the list\n",
    "        embeddings_list.append(embedding)\n",
    "\n",
    "    # Concatenate all the embeddings into a single tensor\n",
    "    deep_embeddings = torch.cat(embeddings_list, dim=0).cpu()\n",
    "\n",
    "    # Save results\n",
    "    torch.save(deep_embeddings, 'data_large_files/poetry_embeddings.pt')\n",
    "\n",
    "else:\n",
    "    # Download pre-generated\n",
    "    # Google Drive file link and output path\n",
    "    google_drive_link = \"https://drive.google.com/uc?id=18z1pVoJbl66gB7HdkXxa7-UOzi99y764\"  # noqa\n",
    "    output_path = 'data_large_files/poetry_embeddings.pt'\n",
    "\n",
    "    # Check if the file already exists before downloading\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"File not found locally. Downloading from Google Drive to {output_path}...\")\n",
    "        gdown.download(google_drive_link, output_path, quiet=False)\n",
    "    else:\n",
    "        print(f\"File already exists at {output_path}. Skipping download.\")\n",
    "\n",
    "    # Load the embeddings after download or if already present\n",
    "    deep_embeddings = torch.load(output_path, weights_only=True)\n",
    "\n",
    "# Change to numpy\n",
    "deep_embeddings = deep_embeddings.numpy()\n",
    "print(\"Embedding shape:\", deep_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b6e3f-7547-4f25-be7f-e22842705bae",
   "metadata": {},
   "source": [
    "### FAISS Index\n",
    "\n",
    "Now we create the FAISS index. There are many different methods we can use, and typical ones are listed below:\n",
    "\n",
    "\n",
    "| Name          | Speed (1-5) | Memory (1-5) | Accuracy (1-5) | ANN* (Yes/No) | Description                                                                      |\n",
    "|---------------|-------------|--------------|----------------|---------------|----------------------------------------------------------------------------------|\n",
    "| IndexFlatL2   | 2           | 5            | 5              | No            | Direct computation of distances; exact nearest neighbor search.                  |\n",
    "| IndexIVFFlat  | 4           | 3            | 3              | Yes           | Uses inverted file indexing to search within clusters, improving speed.          |\n",
    "| IndexIVFPQ    | 5           | 2            | 3              | Yes           | Combines IVF with product quantization for fast and memory-efficient search.     |\n",
    "| IndexHNSWFlat | 5           | 4            | 4              | Yes           | Utilizes a hierarchical navigable small world graph for efficient searching.     |\n",
    "| IndexLSH      | 5           | 2            | 2              | Yes           | Employs locality-sensitive hashing for fast approximate nearest neighbor search. |\n",
    "\n",
    "* **ANN** stands for **Approximate Nearest Neighbor**. This refers to algorithms that find points in a high-dimensional space that are close to a given query point but do not guarantee exact matches. ANN methods trade off some accuracy for improved speed and efficiency, making them suitable for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95786d8-8d76-493a-8d72-000ede05f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HNSW FAISS index\n",
    "embedding_dim = deep_embeddings.shape[1]  # Dimension of your embeddings\n",
    "num_neighbors = 32  # Number of connections (neighbors) for each node\n",
    "index = faiss.IndexHNSWFlat(embedding_dim, num_neighbors)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(deep_embeddings)\n",
    "\n",
    "# Save index to file\n",
    "faiss.write_index(index, \"knowledge_bases/poetry_faiss.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae9f74-ea94-4ab1-9223-6f830728de24",
   "metadata": {},
   "source": [
    "Perform a search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a0f2c-dc82-4e8c-a0cb-67e76fb82f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the nearest neighbors of the first embedding\n",
    "query_embedding = deep_embeddings[0:1]\n",
    "\n",
    "# k is number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_embedding, k=5)\n",
    "\n",
    "# Step 4: Display the results\n",
    "print(\"Nearest neighbors (indices):\", indices)\n",
    "print(\"Distances to the nearest neighbors:\", distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a3bce-c311-4f9b-a8d8-07a5343f6fb2",
   "metadata": {},
   "source": [
    "# Knowledge Graph\n",
    "\n",
    "In addition to the FAISS index, we will also create a keyword-based knowledge graph using **NetworkX**, a powerful graph database management system.\n",
    "\n",
    "First, we will identify up to ten keywords from each poem, which will serve as the nodes in our knowledge graph. The connections between two keywords will represent their co-occurrence in the poems. This knowledge graph not only enables fast retrieval by keyword but also facilitates **associative retrieval**, expanding the scope of our searches.\n",
    "\n",
    "### Keyword Identification\n",
    "\n",
    "We will utilize our LLM to identify keywords through natural language prompts. Additionally, traditional NLP methods such as TF-IDF and RAKE can be employed for this task. While these methods are generally more efficient, they may lack the deeper comprehension needed to uncover nuanced themes, especially those in poems.\n",
    "\n",
    "**Prompts** are crucial for the quality of LLM-based reasoning. In this example, our prompt provides clear instructions along with an example to outline the **goal, requirements, and output format**. For example, the presence of the `[` character at the end is key to achieving a stable format, leveraging the autoregressive nature of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2ffe4-690d-4c3f-a54b-413edfb7bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt_template = (\"Identify or infer up to 10 semantically meaningful keywords from the following poem. \" +\n",
    "                   \"The keywords should be commonly used nouns or verbs. \" +\n",
    "                   \"Provide the keywords directly after `YOUR ANSWER:`, formatted within brackets and separated by commas, such as \" +\n",
    "                   \"YOUR ANSWER: [teacher, classroom].\\n\" +\n",
    "                   \"\\n\\n%s\\n\\n%s\\n\\n\\n\" +\n",
    "                   \"YOUR ANSWER: [\")\n",
    "\n",
    "# A prompt example\n",
    "print(prompt_template % (\"POEM TITLE\", \"POEM BODY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8df22-5c18-4919-95b3-8902182092f3",
   "metadata": {},
   "source": [
    "Again, this process is computationally expensive and may take up to **five hours on an A100 GPU**. Therefore, we provide a pre-generated version available for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db8092-7e5c-484e-ac7e-f15e02daf8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_MASSIVE_COMPUTATION:\n",
    "    # List to store the keywords for each text\n",
    "    keywords_list = []\n",
    "\n",
    "    # Loop through the texts and display a progress bar using tqdm.notebook.trange\n",
    "    pbar = trange(len(df), desc=\"Extracting Keywords\")\n",
    "    for i in pbar:\n",
    "        # Create a prompt\n",
    "        prompt = prompt_template % (df.iloc[i]['Title'], df.iloc[i]['Poem'])\n",
    "\n",
    "        # Encode the prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate output\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs['input_ids'].cuda(),\n",
    "                                     max_new_tokens=25, temperature=0.1,\n",
    "                                     attention_mask=inputs['attention_mask'],\n",
    "                                     pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        # Contents after \"ANSWER:\"\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"YOUR ANSWER:\")[-1].strip()\n",
    "\n",
    "        # Parsing results\n",
    "        first_left_index = generated_text.find(\"[\")\n",
    "        first_right_index = generated_text.find(\"]\")\n",
    "        keywords_text = generated_text[first_left_index + 1:first_right_index]\n",
    "        keywords = [keyword.strip() for keyword in keywords_text.split(\",\") if keyword.strip()]\n",
    "\n",
    "        # Store in results\n",
    "        keywords_list.append(keywords)\n",
    "        pbar.set_postfix(keywords=str(keywords))\n",
    "\n",
    "    # Save keywords_list to a pickle file\n",
    "    with open('knowledge_bases/poetry_keywords.pkl', 'wb') as f:\n",
    "        pickle.dump(keywords_list, f)\n",
    "else:\n",
    "    # Load the keywords after download or if already present\n",
    "    with open('knowledge_bases/poetry_keywords.pkl', 'rb') as f:\n",
    "        keywords_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0156e-b991-4c50-b928-e121c448f0c1",
   "metadata": {},
   "source": [
    "Next, we will parse the results using **spaCy**, a powerful tool for performing fine-scale NLP tasks.\n",
    "\n",
    "1. Retain only the nouns and verbs.\n",
    "2. Convert them to their lemmatized form (e.g., `eggs` to `egg`, and `ran` to `run`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93f71d-98c0-4d38-9695-50dd79fea89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# List to store the cleaned keywords\n",
    "cleaned_keywords_list = []\n",
    "\n",
    "# Process each list of keywords individually\n",
    "for keywords in tqdm(keywords_list, \"Parsing keywords\"):\n",
    "    filtered_keywords = []\n",
    "    for keyword in keywords:\n",
    "        # Process each keyword individually\n",
    "        doc = nlp(keyword)\n",
    "        if doc[0].pos_ in ['NOUN', 'PROPN', 'VERB']:  # filter\n",
    "            filtered_keywords.append(doc[0].lemma_)  # lemmatize\n",
    "\n",
    "    # Append the cleaned keywords to the list\n",
    "    cleaned_keywords_list.append(filtered_keywords)\n",
    "\n",
    "# Output the cleaned keywords\n",
    "for i in range(5):\n",
    "    print(\"Before:\", keywords_list[i])\n",
    "    print(\"After: \", cleaned_keywords_list[i])\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf3fd1-837d-4b1a-900d-bd8d048257df",
   "metadata": {},
   "source": [
    "Next, we will determine a set of unique keywords and establish the forward and inverse mappings:\n",
    "\n",
    "* **Forward Mapping**: Maps poems to their corresponding keywords.\n",
    "* **Inverse Mapping**: Maps keywords to the poems in which they appear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c0b7a-ff90-4e66-a975-58ce3e129793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the cleaned keywords and get unique keywords\n",
    "flattened_keywords = [kw for sublist in cleaned_keywords_list for kw in sublist]\n",
    "unique_keywords_list = list(set(flattened_keywords))\n",
    "\n",
    "# Create a mapping from keyword to its index for O(1) lookups\n",
    "keyword_to_index = {kw: idx for idx, kw in enumerate(unique_keywords_list)}\n",
    "\n",
    "# Initialize inverse mapping\n",
    "inverse_mapping = [[] for _ in range(len(unique_keywords_list))]\n",
    "\n",
    "# Initialize forward mapping with the same structure as inverse_mapping\n",
    "forward_mapping = [[] for _ in range(len(cleaned_keywords_list))]\n",
    "\n",
    "# Create forward and inverse mappings in one loop\n",
    "for poem_index, keywords in enumerate(cleaned_keywords_list):\n",
    "    for kw in keywords:\n",
    "        if kw in keyword_to_index:  # Check if the keyword exists in the mapping\n",
    "            keyword_index = keyword_to_index[kw]\n",
    "            forward_mapping[poem_index].append(keyword_index)  # Pop forward mapping\n",
    "            inverse_mapping[keyword_index].append(poem_index)  # Pop inverse mapping\n",
    "\n",
    "# Output the results\n",
    "print(\"Number of Unique Keywords:\", len(unique_keywords_list))\n",
    "print(\"Unique Keywords List:\", unique_keywords_list[:3])\n",
    "print(\"Forward Mapping:\", forward_mapping[:3])\n",
    "print(\"Inverse Mapping:\", inverse_mapping[:3])\n",
    "\n",
    "# Save the results\n",
    "with open('knowledge_bases/poetry_unique_keywords.pkl', 'wb') as f:\n",
    "    pickle.dump(unique_keywords_list, f)\n",
    "with open('knowledge_bases/poetry_forward_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(forward_mapping, f)\n",
    "with open('knowledge_bases/poetry_inverse_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(inverse_mapping, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fee0990-075a-4b79-97ac-fbdef7da1ea0",
   "metadata": {},
   "source": [
    "### Graph Creation\n",
    "\n",
    "Now we are ready to create a graph of the keywords using **NetworkX**. A graph consists of nodes and edges:\n",
    "\n",
    "* **Nodes**: Each keyword will be represented as a node.\n",
    "* **Edges**: The edge between two nodes is defined by their co-occurrence in poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6a90a-a5fc-477d-868a-7bef5e9d4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "kw_graph = nx.Graph()\n",
    "\n",
    "# Add nodes for each unique keyword\n",
    "kw_graph.add_nodes_from(unique_keywords_list)\n",
    "\n",
    "# Add edges based on co-occurrence\n",
    "num_keywords = len(unique_keywords_list)\n",
    "for i in trange(num_keywords, desc=\"Forming edges\"):\n",
    "    for j in range(i + 1, num_keywords):  # Avoid duplicate edges\n",
    "        co_occur = len(set(inverse_mapping[i]) & set(inverse_mapping[j]))\n",
    "        if co_occur > 0:\n",
    "            kw_graph.add_edge(unique_keywords_list[i], unique_keywords_list[j], weight=co_occur)\n",
    "\n",
    "# Save the graph and mapping\n",
    "nx.write_gpickle(kw_graph, \"knowledge_bases/poetry_keyword_graph.gpickle\")  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca8709-8d0f-49fc-90dc-532569dfb17f",
   "metadata": {},
   "source": [
    "Let's visualize the top K nodes of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b28413-94ff-42f9-ac7f-6f40945a4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many nodes to visualize\n",
    "k_nodes = 20\n",
    "\n",
    "# Compute degree centrality of graph\n",
    "degree_centrality = nx.degree_centrality(kw_graph)\n",
    "\n",
    "# Sort nodes by centrality and take the top K\n",
    "top_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:k_nodes]\n",
    "top_nodes_set = set(node for node, _ in top_nodes)\n",
    "\n",
    "# Create a subgraph with only the top nodes and their edges\n",
    "subgraph = kw_graph.subgraph(top_nodes_set)\n",
    "\n",
    "# Create a layout for the subgraph\n",
    "pos = nx.spring_layout(subgraph)\n",
    "\n",
    "# Draw the subgraph\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw_networkx_nodes(subgraph, pos, node_size=700, node_color='skyblue', alpha=0.7)\n",
    "nx.draw_networkx_edges(subgraph, pos, width=1, alpha=0.5, edge_color='gray')\n",
    "nx.draw_networkx_labels(subgraph, pos, font_size=12)\n",
    "plt.title(f\"Top {k_nodes} Nodes by Degree Centrality\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Create a dictionary to store the number of neighbors for each node\n",
    "neighbors_count = {node: len(list(kw_graph.neighbors(node))) for node, _ in top_nodes}\n",
    "\n",
    "# Print the first ten nodes with the most neighbors\n",
    "print(f\"Top {k_nodes} nodes with the most neighbors:\")\n",
    "for node, count in neighbors_count.items():\n",
    "    print(f\"{node}: {count} neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31da849a-23e3-4610-87cb-81bf8ab8d6e3",
   "metadata": {},
   "source": [
    "Finally, let's perform pathfinding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d611b8-d396-4966-9d84-288f0867caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths(graph, start_node, length=2, num_paths=1, shortest=True):\n",
    "    \"\"\"\n",
    "    Find paths in a graph.\n",
    "\n",
    "    Parameters:\n",
    "    - graph: The NetworkX graph to search.\n",
    "    - start_node: The starting node for the paths.\n",
    "    - length: The length of the path (including start).\n",
    "    - num_paths: The number of paths to return.\n",
    "    - shortest: If true, select the nearest neighbors based on edge weights; otherwise, random.\n",
    "\n",
    "    Returns:\n",
    "    - A list of paths, each represented as a list of node names.\n",
    "    \"\"\"\n",
    "    all_paths = []\n",
    "\n",
    "    while len(all_paths) < num_paths:\n",
    "        current_path = [start_node]\n",
    "        current_node = start_node\n",
    "\n",
    "        # Build a path based on the method chosen\n",
    "        while len(current_path) < length:\n",
    "            neighbors = list(graph.neighbors(current_node))\n",
    "            if not neighbors:\n",
    "                break  # No more neighbors to explore\n",
    "\n",
    "            if shortest:\n",
    "                # Choose the nearest neighbor based on edge weight\n",
    "                next_node = max(neighbors, key=lambda n: graph[current_node][n].get('weight', 0))\n",
    "            else:\n",
    "                next_node = np.random.choice(neighbors)  # Random selection\n",
    "\n",
    "            current_path.append(next_node)\n",
    "            current_node = next_node\n",
    "\n",
    "        if len(current_path) == length:\n",
    "            all_paths.append(current_path)\n",
    "\n",
    "    return all_paths\n",
    "\n",
    "\n",
    "# Perform pathfinding\n",
    "print(\"Shortest\", find_paths(kw_graph, \"hill\", length=3, num_paths=2, shortest=True))\n",
    "print(\"Random\", find_paths(kw_graph, \"hill\", length=3, num_paths=2, shortest=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd4eb6-23d7-4dfd-b159-e3a8498781e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
