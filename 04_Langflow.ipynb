{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28b3b2c2d2ddad8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any\n",
    "\n",
    "import requests\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.schema import LLMResult, Generation\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langflow.base.models.model import LCModelComponent\n",
    "from langflow.field_typing import LanguageModel\n",
    "from langflow.inputs import (\n",
    "    StrInput,\n",
    "    FloatInput,\n",
    "    IntInput,\n",
    ")\n",
    "\n",
    "\n",
    "class CustomLLMComponent(LCModelComponent):\n",
    "    display_name = \"My LLM\"\n",
    "    description = \"Generates text using a custom LLM server.\"\n",
    "    icon = \"Heart\"\n",
    "    name = \"MyLLMModel\"\n",
    "\n",
    "    inputs = LCModelComponent._base_inputs + [\n",
    "        StrInput(\n",
    "            name=\"llm_server_url\",\n",
    "            display_name=\"LLM Server URL\",\n",
    "            advanced=False,\n",
    "            info=\"URL for the custom LLM server.\",\n",
    "            value=\"https://ABC.loca.lt\",\n",
    "        ),\n",
    "        FloatInput(\n",
    "            name=\"temperature\",\n",
    "            display_name=\"Temperature\",\n",
    "            value=0.7,\n",
    "            info=\"Sampling temperature for text generation.\",\n",
    "            advanced=False,\n",
    "        ),\n",
    "        IntInput(\n",
    "            name=\"max_new_tokens\",\n",
    "            display_name=\"Max New Tokens\",\n",
    "            value=50,\n",
    "            info=\"Maximum number of tokens to generate.\",\n",
    "            advanced=False,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    def build_model(self) -> LanguageModel:\n",
    "        # Instantiate CustomLLM with the appropriate parameters\n",
    "        return self.CustomLLM(\n",
    "            llm_server_url=(self.llm_server_url or \"https://ABC.loca.lt\") + \"/generate_llama3\",\n",
    "            temperature=self.temperature,\n",
    "            max_new_tokens=self.max_new_tokens\n",
    "        )\n",
    "\n",
    "    class CustomLLM(BaseLLM):\n",
    "        \"\"\"Wrapper class for custom LLM model to comply with the LanguageModel interface.\"\"\"\n",
    "\n",
    "        # Define fields as class-level variables\n",
    "        llm_server_url: str\n",
    "        temperature: float\n",
    "        max_new_tokens: int\n",
    "\n",
    "        def _call(\n",
    "                self,\n",
    "                prompt: str,\n",
    "                stop: Optional[List[str]] = None,  # noqa\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,  # noqa\n",
    "                **kwargs: Any,  # noqa\n",
    "        ) -> str:\n",
    "            \"\"\"Generate text from the custom LLM model.\"\"\"\n",
    "            payload = {\n",
    "                \"text\": prompt,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"max_new_tokens\": self.max_new_tokens\n",
    "            }\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            try:\n",
    "                response = requests.post(self.llm_server_url, json=payload, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                result = response.json()\n",
    "                return result.get(\"generated_text\", \"No generated text returned.\")\n",
    "            except requests.RequestException as e:\n",
    "                return f\"Error generating text: {e}\"\n",
    "\n",
    "        def _generate(\n",
    "                self,\n",
    "                prompts: List[str],\n",
    "                stop: Optional[List[str]] = None,\n",
    "                run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "                **kwargs: Any,\n",
    "        ) -> LLMResult:\n",
    "            \"\"\"Implements the required _generate method to handle batch generation.\"\"\"\n",
    "            generations = []\n",
    "            for prompt in prompts:\n",
    "                text = self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
    "                generations.append([Generation(text=text)])\n",
    "\n",
    "            return LLMResult(generations=generations)\n",
    "\n",
    "        @property\n",
    "        def _llm_type(self) -> str:\n",
    "            return \"custom_llm\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dfa2d07fd27b51a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langflow.custom import Component\n",
    "from langflow.io import MessageTextInput, Output\n",
    "from langflow.schema.message import Message\n",
    "\n",
    "class SingleTurnFilterComponent(Component):\n",
    "    display_name = \"Single-turn Filter\"\n",
    "    description = \"Filters the input to keep only the initial response, removing any multi-turn conversation generated (e.g., '\\\\nUser:', '\\\\nAI:').\"\n",
    "    icon = \"scissors-line-dashed\"\n",
    "    name = \"SingleTurnFilter\"\n",
    "\n",
    "    inputs = [\n",
    "        MessageTextInput(\n",
    "            name=\"text\",\n",
    "            display_name=\"Text\",\n",
    "            info=\"The input text containing conversation segments.\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    outputs = [\n",
    "        Output(display_name=\"Filtered Response\", name=\"filtered_response\", method=\"filter\"),\n",
    "    ]\n",
    "\n",
    "    def filter(self) -> Message:\n",
    "        # Access the input text directly\n",
    "        text = self.text\n",
    "        \n",
    "        # Remove any multi-turn conversation markers, keeping only the initial response\n",
    "        filtered_response = text.split(\"\\nUser:\")[0].split(\"\\nAI:\")[0]\n",
    "        \n",
    "        # Set the component status and return the filtered response as a Message\n",
    "        self.status = filtered_response\n",
    "        return Message(text=filtered_response)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa4f23fd770248d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6795a47a23442518"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
