{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bb3efd6d1794a01b66c27b867409c2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93a79789eaf4404cbb1e66680275d347",
       "IPY_MODEL_f4a1e41696564260af9edf4c87f120ba",
       "IPY_MODEL_ab99f154499641c7b16a724d09409956"
      ],
      "layout": "IPY_MODEL_4f8eae8335f147bbab1107f619f30d24"
     }
    },
    "93a79789eaf4404cbb1e66680275d347": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f43a26e911f4f46b9802186942fe408",
      "placeholder": "​",
      "style": "IPY_MODEL_89c6f44f7f2c4adca32af7d6f3a58452",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "f4a1e41696564260af9edf4c87f120ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5cb7c7eb6ef4069bd13099582cf5453",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13d2c8aca673481184654e1af26a2b73",
      "value": 2
     }
    },
    "ab99f154499641c7b16a724d09409956": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d6eb16f5f5a4a5ca7d2f2caa21d4b5d",
      "placeholder": "​",
      "style": "IPY_MODEL_801549846c88431792724812adc8e408",
      "value": " 2/2 [00:47&lt;00:00, 23.18s/it]"
     }
    },
    "4f8eae8335f147bbab1107f619f30d24": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f43a26e911f4f46b9802186942fe408": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89c6f44f7f2c4adca32af7d6f3a58452": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5cb7c7eb6ef4069bd13099582cf5453": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13d2c8aca673481184654e1af26a2b73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0d6eb16f5f5a4a5ca7d2f2caa21d4b5d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "801549846c88431792724812adc8e408": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlBmfOVOoYaV",
    "outputId": "51581743-08bf-40dd-df79-c20e6f39f9ca",
    "ExecuteTime": {
     "end_time": "2024-10-21T19:32:09.522660Z",
     "start_time": "2024-10-21T19:32:05.969635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (4.40.0.dev0)\r\n",
      "Requirement already satisfied: filelock in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (3.13.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (0.22.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (2.32.2)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (4.66.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "Requirement already satisfied: accelerate in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (0.29.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from accelerate) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from accelerate) (24.0)\r\n",
      "Requirement already satisfied: psutil in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from accelerate) (5.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from accelerate) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from accelerate) (2.2.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from accelerate) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from accelerate) (0.4.2)\r\n",
      "Requirement already satisfied: filelock in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\r\n",
      "Requirement already satisfied: requests in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.32.2)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: bitsandbytes in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (0.42.0)\r\n",
      "Requirement already satisfied: scipy in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from bitsandbytes) (1.13.0)\r\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /Users/kuangdai/anaconda3/envs/llm/lib/python3.11/site-packages (from scipy->bitsandbytes) (1.26.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install -U bitsandbytes\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load model and tokenizer"
   ],
   "metadata": {
    "id": "rDHPkZbKvVgo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using LLaMA for This Notebook\n",
    "\n",
    "1. **Apply for model access**: Visit [LLaMA-2-7B-Chat on Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) to request access to the model. Please note that it may take a few days for your application to be approved. Once approved, you will see the following message on the website:\n",
    "\n",
    "    > **Gated model** You have been granted access to this model\n",
    "   \n",
    "2. **Create your Hugging Face Access Key**: Go to your [Hugging Face settings](https://huggingface.co/settings/tokens) to create an access token. When creating the token, ensure you check the box:\n",
    "\n",
    "    - `Read access to contents of all public gated repos you can access` under **Permissions**.\n",
    "\n",
    "3. **Provide your Hugging Face Access Key**: Once you have your access token, paste it below to authenticate the notebook with Hugging Face."
   ],
   "metadata": {
    "id": "R4VS8C_0t4EJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "hf_access_key = \"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\""
   ],
   "metadata": {
    "id": "iIjKW_57pjjO",
    "ExecuteTime": {
     "end_time": "2024-10-21T19:32:10.763759Z",
     "start_time": "2024-10-21T19:32:10.760405Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quantization\n",
    "\n",
    "Using free Colab, you have access to **16 GB of GPU memory**, which is insufficient to load the entire LLaMA model at once. To complete inference, Hugging Face will dynamically move parts of the model onto the GPU during runtime, which will cause the inference to become **extremely slow**.\n",
    "\n",
    "To address this limitation, we **quantize** the model using Hugging Face's `bitsandbytes` library. This approach significantly reduces GPU memory consumption, enabling faster inference without needing to load the entire model into GPU memory at once.\n"
   ],
   "metadata": {
    "id": "xGtNsN8DRjOK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a BitsAndBytesConfig for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,       # Optional for performance\n",
    "    bnb_4bit_quant_type='nf4',            # Normal floating-point 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Set compute dtype to float16 for faster inference\n",
    ")"
   ],
   "metadata": {
    "id": "K9prcxTERlZz",
    "ExecuteTime": {
     "end_time": "2024-10-21T19:32:11.551153Z",
     "start_time": "2024-10-21T19:32:11.544530Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, load the model and tokenizer."
   ],
   "metadata": {
    "id": "xBrEpszVTfDS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_access_key,\n",
    "    quantization_config=quantization_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, token=hf_access_key)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2bb3efd6d1794a01b66c27b867409c2d",
      "93a79789eaf4404cbb1e66680275d347",
      "f4a1e41696564260af9edf4c87f120ba",
      "ab99f154499641c7b16a724d09409956",
      "4f8eae8335f147bbab1107f619f30d24",
      "3f43a26e911f4f46b9802186942fe408",
      "89c6f44f7f2c4adca32af7d6f3a58452",
      "f5cb7c7eb6ef4069bd13099582cf5453",
      "13d2c8aca673481184654e1af26a2b73",
      "0d6eb16f5f5a4a5ca7d2f2caa21d4b5d",
      "801549846c88431792724812adc8e408"
     ]
    },
    "id": "4HXJBeHCohOX",
    "outputId": "ff803656-8755-400f-8a0a-902c6365d6be",
    "ExecuteTime": {
     "end_time": "2024-10-21T19:32:12.747595Z",
     "start_time": "2024-10-21T19:32:12.562477Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m      3\u001B[0m     model_name,\n\u001B[1;32m      4\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m     token\u001B[38;5;241m=\u001B[39mhf_access_key,\n\u001B[1;32m      6\u001B[0m     quantization_config\u001B[38;5;241m=\u001B[39mquantization_config)\n\u001B[1;32m      7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name, use_fast\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, token\u001B[38;5;241m=\u001B[39mhf_access_key)\n",
      "File \u001B[0;32m~/stfc/twiker/transformers-twiker/src/transformers/models/auto/auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    561\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    562\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 563\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    564\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    565\u001B[0m     )\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    567\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m )\n",
      "File \u001B[0;32m~/stfc/twiker/transformers-twiker/src/transformers/modeling_utils.py:3165\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3162\u001B[0m     hf_quantizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 3165\u001B[0m     hf_quantizer\u001B[38;5;241m.\u001B[39mvalidate_environment(\n\u001B[1;32m   3166\u001B[0m         torch_dtype\u001B[38;5;241m=\u001B[39mtorch_dtype, from_tf\u001B[38;5;241m=\u001B[39mfrom_tf, from_flax\u001B[38;5;241m=\u001B[39mfrom_flax, device_map\u001B[38;5;241m=\u001B[39mdevice_map\n\u001B[1;32m   3167\u001B[0m     )\n\u001B[1;32m   3168\u001B[0m     torch_dtype \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_torch_dtype(torch_dtype)\n\u001B[1;32m   3169\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_device_map(device_map)\n",
      "File \u001B[0;32m~/stfc/twiker/transformers-twiker/src/transformers/quantizers/quantizer_bnb_4bit.py:62\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidate_environment\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_accelerate_available() \u001B[38;5;129;01mand\u001B[39;00m is_bitsandbytes_available()):\n\u001B[0;32m---> 62\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m     63\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     65\u001B[0m         )\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_tf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_flax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     69\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     70\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m sure the weights are in PyTorch format.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     71\u001B[0m         )\n",
      "\u001B[0;31mImportError\u001B[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s check the GPU information and verify that all parts of the model are loaded onto the GPU."
   ],
   "metadata": {
    "id": "ED02SzHZTvNg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Check GPU info\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current GPU memory usage in MB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 2\n",
    "    reserved_memory = torch.cuda.memory_reserved(0) / 1024 ** 2\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1024 ** 2\n",
    "    free_memory = reserved_memory - allocated_memory\n",
    "    print(f\"Total GPU memory: {total_memory:.2f} MB\")\n",
    "    print(f\"Reserved GPU memory: {reserved_memory:.2f} MB\")\n",
    "    print(f\"Allocated GPU memory: {allocated_memory:.2f} MB\")\n",
    "    print(f\"Free GPU memory: {free_memory:.2f} MB\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "# Check the device of each module of the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is on device: {param.device}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDwKD8171SYP",
    "outputId": "1c6f5f2c-c12f-4958-f0e6-bb95e0b55c71"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total GPU memory: 15102.06 MB\n",
      "Reserved GPU memory: 3996.00 MB\n",
      "Allocated GPU memory: 3734.77 MB\n",
      "Free GPU memory: 261.23 MB\n",
      "model.embed_tokens.weight is on device: cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.0.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.0.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.0.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.1.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.1.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.1.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.2.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.2.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.2.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.3.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.3.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.3.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.4.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.4.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.4.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.5.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.5.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.5.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.6.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.6.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.6.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.7.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.7.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.7.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.8.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.8.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.8.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.8.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.8.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.8.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.8.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.8.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.9.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.9.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.9.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.9.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.9.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.9.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.9.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.9.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.9.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.10.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.10.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.10.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.10.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.10.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.10.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.10.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.10.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.10.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.11.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.11.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.11.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.11.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.11.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.11.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.11.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.11.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.11.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.12.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.12.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.12.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.12.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.12.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.12.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.12.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.12.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.12.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.13.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.13.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.13.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.13.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.13.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.13.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.13.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.13.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.13.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.14.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.14.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.14.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.14.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.14.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.14.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.14.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.14.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.14.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.15.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.15.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.15.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.15.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.15.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.15.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.15.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.15.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.15.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.16.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.16.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.16.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.16.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.16.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.16.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.16.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.16.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.16.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.17.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.17.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.17.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.17.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.17.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.17.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.17.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.17.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.17.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.18.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.18.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.18.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.18.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.18.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.18.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.18.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.18.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.18.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.19.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.19.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.19.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.19.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.19.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.19.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.19.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.19.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.19.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.20.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.20.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.20.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.20.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.20.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.20.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.20.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.20.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.20.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.21.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.21.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.21.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.21.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.21.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.21.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.21.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.21.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.21.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.22.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.22.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.22.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.22.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.22.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.22.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.22.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.22.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.22.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.23.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.23.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.23.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.23.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.23.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.23.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.23.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.23.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.23.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.24.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.24.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.24.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.24.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.24.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.24.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.24.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.24.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.24.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.25.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.25.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.25.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.25.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.25.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.25.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.25.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.25.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.25.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.26.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.26.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.26.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.26.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.26.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.26.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.26.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.26.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.26.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.27.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.27.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.27.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.27.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.27.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.27.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.27.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.27.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.27.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.28.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.28.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.28.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.28.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.28.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.28.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.28.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.28.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.28.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.29.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.29.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.29.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.29.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.29.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.29.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.29.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.29.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.29.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.30.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.30.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.30.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.30.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.30.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.30.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.30.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.30.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.30.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.layers.31.self_attn.q_proj.weight is on device: cuda:0\n",
      "model.layers.31.self_attn.k_proj.weight is on device: cuda:0\n",
      "model.layers.31.self_attn.v_proj.weight is on device: cuda:0\n",
      "model.layers.31.self_attn.o_proj.weight is on device: cuda:0\n",
      "model.layers.31.mlp.gate_proj.weight is on device: cuda:0\n",
      "model.layers.31.mlp.up_proj.weight is on device: cuda:0\n",
      "model.layers.31.mlp.down_proj.weight is on device: cuda:0\n",
      "model.layers.31.input_layernorm.weight is on device: cuda:0\n",
      "model.layers.31.post_attention_layernorm.weight is on device: cuda:0\n",
      "model.norm.weight is on device: cuda:0\n",
      "lm_head.weight is on device: cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try the model."
   ],
   "metadata": {
    "id": "IV1-qVCnUWkf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Get the token ID for \"\\n\"\n",
    "newline_token_id = tokenizer(\"\\n\", return_tensors=\"pt\").input_ids[0][0]\n",
    "\n",
    "# Generate chat response\n",
    "def generate_response(prompt, max_length=30, temperature=0.2, num_beams=1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(inputs.input_ids,\n",
    "                             eos_token_id=newline_token_id,  # stop generation if \"\\n\" occurs\n",
    "                             max_length=max_length,\n",
    "                             temperature=temperature,\n",
    "                             num_beams=num_beams)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Try it out\n",
    "generate_response(\"Hi mate!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VGx93sN0pAv9",
    "outputId": "c9e14a78-20ef-4f5d-9c5f-5ec68dfcf10a"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Hi mate! I'm just an AI, I don't have personal experiences, but I can help you with any questions or tasks\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Key parameters in `model.generate()`\n",
    "\n",
    "\n",
    "\n",
    "#### **Length Control**\n",
    "- **`max_length`**: Defines the maximum number of tokens to generate, including both the input and generated tokens. The model will stop once this limit is reached.\n",
    "- **`min_length`**: Sets the minimum number of tokens that must be generated before stopping. This ensures that the output doesn't stop too early.\n",
    "- **`eos_token_id`**: The ID of the end-of-sequence (EOS) token. The generation will stop once the model generates this token, marking the end of the sequence.\n",
    "\n",
    "#### **Diversity and Quality**\n",
    "- **`temperature`**: Controls the randomness of predictions. Lower values (e.g., 0.7) make the model more deterministic, while higher values (e.g., 1.0 or above) increase randomness, making the outputs more diverse.\n",
    "- **`top_k`**: Limits the next token selection to the top `k` most likely tokens. A higher value allows for more variety in the generated text, while a lower value makes it more deterministic.\n",
    "- **`top_p` (nucleus sampling)**: Limits token selection to tokens with a cumulative probability of `p`. This ensures that only the top `p` percent of the probability mass is considered, promoting diverse but controlled generation.\n",
    "- **`do_sample`**: Enables random sampling of tokens instead of greedy decoding (which selects the highest-probability token). This is essential for generating diverse outputs.\n",
    "- **`num_beams`**: The number of beams for beam search. Higher values explore more possibilities during generation, leading to better outputs but at the cost of increased computation.\n",
    "\n",
    "![beam](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "#### **Repetition and Token Constraints**\n",
    "- **`repetition_penalty`**: Penalizes repeated tokens, discouraging the model from generating repetitive sequences. A value greater than 1.0 reduces the likelihood of repeating the same token.\n",
    "- **`no_repeat_ngram_size`**: Prevents repetition of n-grams of a specified size. For example, `no_repeat_ngram_size=3` ensures that trigrams (3-grams) do not repeat in the generated output.\n",
    "\n",
    "#### **Output Control**\n",
    "- **`num_return_sequences`**: The number of different sequences to generate. For example, `num_return_sequences=3` generates three separate outputs from the same prompt.\n"
   ],
   "metadata": {
    "id": "3NBJlpV3X_pj"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "bLuRoV1AOj2_"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "VJ5Yd6M8Oj5o"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # Chat loop with history\n",
    "# print(\"Start chatting! Type 'exit' to end the conversation.\\n\")\n",
    "\n",
    "# # Initialize conversation history\n",
    "# conversation_history = \"\"\n",
    "\n",
    "# while True:\n",
    "#     # Get input from the user\n",
    "#     user_input = input(\"You: \")\n",
    "\n",
    "#     # Exit the chat loop\n",
    "#     if user_input.lower() == \"exit\":\n",
    "#         print(\"Ending chat. Goodbye!\")\n",
    "#         break\n",
    "\n",
    "#     # Append the user input to the conversation history\n",
    "#     conversation_history += f\"You: {user_input}\\n\"\n",
    "\n",
    "#     # Generate response using the updated conversation history\n",
    "#     response = generate_response(conversation_history)\n",
    "\n",
    "#     # Append the model's response to the conversation history\n",
    "#     conversation_history += f\"Bot: {response}\\n\"\n",
    "\n",
    "#     # Print the model's response\n",
    "#     print(f\"Bot: {response}\\n\")"
   ],
   "metadata": {
    "id": "H86yAZu2vvo5"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-Fk2goXdObF-"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "sA2dFrQ4ObIx"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "XjbJ_a_7ObNK"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "rJ_VaGRNObQF"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fJqIke2l3fU3"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "3Zs-iF825Dvq"
   },
   "execution_count": 7,
   "outputs": []
  }
 ]
}
