{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlBmfOVOoYaV",
    "outputId": "51581743-08bf-40dd-df79-c20e6f39f9ca"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import vllm\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDHPkZbKvVgo"
   },
   "source": [
    "# Load tokenizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4VS8C_0t4EJ"
   },
   "source": [
    "### Using LLaMA 3\n",
    "\n",
    "#### Conditions\n",
    "\n",
    "Llama 3 can be used for commercial products, but there are certain requirements and restrictions to follow:\n",
    "\n",
    "1. **Attribution**:  \n",
    "   You must provide a clear and prominent acknowledgment, such as \"Built with Meta Llama 3,\" in all relevant user interfaces, documentation, and webpages.\n",
    "\n",
    "2. **User Threshold**:  \n",
    "   If your product or service utilizing Llama 3 exceeds **700 million monthly active users**, you must obtain a separate, specific license from Meta.\n",
    "\n",
    "3. **Restrictions on Enhancing Other Models**:  \n",
    "   Llama 3 materials or outputs cannot be used to improve or train any other large language models outside the Llama family.\n",
    "\n",
    "4. **Compliance**:  \n",
    "   Users must ensure compliance with applicable laws and regulations, such as GDPR and trade compliance laws.\n",
    "\n",
    "In conclusion, Llama 3 offers great flexibility for commercial applications, provided you adhere to these licensing terms and restrictions. See [Llama 3 Overview](https://ai.meta.com/static-resource/july-responsible-use-guide).\n",
    "\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. **Apply for model access**: Visit [Llama-3-8B-Instruct on Hugging Face](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) to request access to the model. Please note that it may take a few days for your application to be approved. Once approved, you will see the following message on the website:\n",
    "\n",
    "    * **Gated model** You have been granted access to this model\n",
    "   \n",
    "2. **Create your Hugging Face Access Key**: Go to your [Hugging Face settings](https://huggingface.co/settings/tokens) to create an access token. When creating the token, ensure you check the box:\n",
    "\n",
    "    * `Read access to contents of all public gated repos you can access` under **Permissions**.\n",
    "\n",
    "3. **Provide your Hugging Face Access Key**: Once you have your access token, paste it below to authenticate the notebook with Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIjKW_57pjjO"
   },
   "outputs": [],
   "source": [
    "hf_access_key = \"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\"  # noqa\n",
    "login(hf_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGtNsN8DRjOK"
   },
   "source": [
    "### Quantization\n",
    "\n",
    "Suppose you have access to **16 GB of GPU memory**, which is insufficient to load the entire LLaMA model at once. To complete inference, Hugging Face will dynamically move parts of the model onto the GPU during runtime, which will cause the inference to become **extremely slow**.\n",
    "\n",
    "To address this limitation, we **quantize** the model using Hugging Face's `bitsandbytes` library. This approach significantly reduces GPU memory consumption, enabling faster inference without needing to load the entire model into GPU memory at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9prcxTERlZz"
   },
   "outputs": [],
   "source": [
    "# Create a BitsAndBytesConfig for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Change this to `False` to disable quantization\n",
    "    bnb_4bit_use_double_quant=True,  # Optional for performance\n",
    "    bnb_4bit_quant_type='nf4',  # Normal floating-point 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Set compute dtype to float16 for faster inference\n",
    ")\n",
    "\n",
    "# Model name--you can change to many huggingface models\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBrEpszVTfDS"
   },
   "source": [
    "Now, let's load the tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2bb3efd6d1794a01b66c27b867409c2d",
      "93a79789eaf4404cbb1e66680275d347",
      "f4a1e41696564260af9edf4c87f120ba",
      "ab99f154499641c7b16a724d09409956",
      "4f8eae8335f147bbab1107f619f30d24",
      "3f43a26e911f4f46b9802186942fe408",
      "89c6f44f7f2c4adca32af7d6f3a58452",
      "f5cb7c7eb6ef4069bd13099582cf5453",
      "13d2c8aca673481184654e1af26a2b73",
      "0d6eb16f5f5a4a5ca7d2f2caa21d4b5d",
      "801549846c88431792724812adc8e408"
     ]
    },
    "id": "4HXJBeHCohOX",
    "outputId": "ff803656-8755-400f-8a0a-902c6365d6be"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ED02SzHZTvNg"
   },
   "source": [
    "Letâ€™s check the GPU information and verify that all parts of the model are loaded onto the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDwKD8171SYP",
    "outputId": "1c6f5f2c-c12f-4958-f0e6-bb95e0b55c71",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def info_gpu():\n",
    "    \"\"\" Check GPU info \"\"\"\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1024 ** 3\n",
    "    free_memory = total_memory - allocated_memory\n",
    "    print(f\"Total GPU memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Allocated GPU memory: {allocated_memory:.2f} GB\")\n",
    "    print(f\"Free GPU memory: {free_memory:.2f} GB\")\n",
    "\n",
    "\n",
    "# Check GPU info\n",
    "info_gpu()\n",
    "\n",
    "# Check the device of each module of the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is on device: {param.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV1-qVCnUWkf"
   },
   "source": [
    "Let's try the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VGx93sN0pAv9",
    "outputId": "c9e14a78-20ef-4f5d-9c5f-5ec68dfcf10a"
   },
   "outputs": [],
   "source": [
    "# Generate chat response\n",
    "def generate_response(prompt, max_length=30, temperature=0.2, num_beams=1):\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the output ids\n",
    "    outputs = model.generate(inputs.input_ids,\n",
    "                             attention_mask=inputs['attention_mask'],  # Avoid warning\n",
    "                             pad_token_id=tokenizer.eos_token_id,  # Avoid warning\n",
    "                             max_length=max_length,  # Length of generation\n",
    "                             temperature=temperature,  # Temperature for randomness\n",
    "                             num_beams=num_beams  # Number of beams\n",
    "                             )\n",
    "\n",
    "    # Decode the output ids to a string\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Try it\n",
    "generate_response(\"Hi mate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NBJlpV3X_pj"
   },
   "source": [
    "### Key parameters in `model.generate()`\n",
    "\n",
    "There are numerous arguments for `model.generate()`. Below, we highlight the most useful ones.\n",
    "\n",
    "#### **Length Control**\n",
    "- **`max_length`**: Defines the maximum number of tokens to generate, including both the input and generated tokens. The model will stop once this limit is reached.\n",
    "- **`min_length`**: Sets the minimum number of tokens that must be generated before stopping. This ensures that the output doesn't stop too early.\n",
    "- **`eos_token_id`**: The ID of the end-of-sequence (EOS) token. The generation will stop once the model generates this token, marking the end of the sequence.\n",
    "\n",
    "#### **Diversity and Quality**\n",
    "- **`temperature`**: Controls the randomness of predictions. Lower values (e.g., 0.7) make the model more deterministic, while higher values (e.g., 1.0 or above) increase randomness, making the outputs more diverse.\n",
    "- **`top_k`**: Limits the next token selection to the top `k` most likely tokens. A higher value allows for more variety in the generated text, while a lower value makes it more deterministic.\n",
    "- **`top_p` (nucleus sampling)**: Limits token selection to tokens with a cumulative probability of `p`. This ensures that only the top `p` percent of the probability mass is considered, promoting diverse but controlled generation.\n",
    "- **`do_sample`**: Enables random sampling of tokens instead of greedy decoding (which selects the highest-probability token). This is essential for generating diverse outputs.\n",
    "- **`num_beams`**: The number of beams for beam search. Higher values explore more possibilities during generation, leading to better outputs but at the cost of increased computation.\n",
    "\n",
    "![beam](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "#### **Repetition and Token Constraints**\n",
    "- **`repetition_penalty`**: Penalizes repeated tokens, discouraging the model from generating repetitive sequences. A value greater than 1.0 reduces the likelihood of repeating the same token.\n",
    "- **`no_repeat_ngram_size`**: Prevents repetition of n-grams of a specified size. For example, `no_repeat_ngram_size=3` ensures that trigrams do not repeat in the generated output.\n",
    "\n",
    "#### **Output Control**\n",
    "- **`num_return_sequences`**: The number of different sequences to generate. For example, `num_return_sequences=3` generates three separate outputs from the same prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batch inference\n",
    "\n",
    "Now, we enhance our `generate_response()` function to support batch inference, which is crucial for serving multiple users in production environments. Additionally, we add functionality to measure the runtime of the inference process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJ5Yd6M8Oj5o"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompts, max_length=30, temperature=0.2, num_beams=1, measure_time=False):\n",
    "    # Check if input is a single prompt or batch of prompts\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]  # Convert single prompt to a list for batch processing\n",
    "\n",
    "    # Tokenize the batch of prompts (single or multiple)\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "    # Start time measurement for model.generate() if requested\n",
    "    start_time = time.time() if measure_time else None\n",
    "\n",
    "    # Generate responses for the batch\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs['attention_mask'],  # Avoid warning\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Avoid warning\n",
    "        max_length=max_length,  # Length of generation\n",
    "        temperature=temperature,  # Temperature for randomness\n",
    "        num_beams=num_beams  # Number of beams\n",
    "    )\n",
    "\n",
    "    # Measure time after generation\n",
    "    if measure_time:\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "    else:\n",
    "        runtime = None\n",
    "\n",
    "    # Decode the batch of generated outputs\n",
    "    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    # If the original input was a single prompt, return a single string, not a list\n",
    "    if len(prompts) == 1:\n",
    "        responses = responses[0]\n",
    "\n",
    "    # Return both the response and runtime if time measurement was requested\n",
    "    return (responses, runtime) if measure_time else responses"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For batched inference, padding is necessary to ensure that all input sequences in a batch are of the same length. This allows the model to process multiple inputs in parallel. To set up padding correctly, we need to configure the tokenizer to handle padding. Specifically, we can assign a padding token (typically the `eos_token`) and set the padding side to ensure proper alignment of input sequences.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the pad token to the eos token \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set padding side to left for decoder-only models\n",
    "tokenizer.padding_side = \"left\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's create a multi-input task, and measure its runtime."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch input\n",
    "many_prompts = [\n",
    "    \"Once upon a time in a distant land...\",\n",
    "    \"Explain the theory of relativity in simple terms.\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Generate a story about a robot in the future.\",\n",
    "    \"How do you bake a chocolate cake?\"\n",
    "]\n",
    "\n",
    "# Batch inference\n",
    "many_responses, t = generate_response(model, many_prompts, measure_time=True)\n",
    "\n",
    "# Print results\n",
    "for res in many_responses:\n",
    "    print(\"---------------\")\n",
    "    print(res)\n",
    "print(\"\\n\\nRuntime before acceleration:\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accelerate inference\n",
    "\n",
    "In this section, we utilize `vLLM` to optimize inference through paged attention, as described in [this paper](https://arxiv.org/abs/2309.06180). \n",
    "\n",
    "Before proceeding, we release the GPU memory by deleting the previous model, ensuring efficient usage of resources on a small GPU.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model and free its GPU memory\n",
    "try:\n",
    "    del model  # Deletes the model object\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Optionally, clear the memory cache on the GPU\n",
    "torch.cuda.empty_cache()\n",
    "# Collect garbage to ensure all references are removed\n",
    "gc.collect()\n",
    "# Ensure all CUDA operations are finished before clearing memory (optional)\n",
    "torch.cuda.synchronize()\n",
    "# Check GPU info\n",
    "info_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompts, max_length=30, temperature=0.2, measure_time=False):\n",
    "    # Check if input is a single prompt or batch of prompts\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]  # Convert single prompt to a list for batch processing\n",
    "\n",
    "    # Tokenize the batch of prompts (single or multiple)\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "    # Start time measurement for model.generate() if requested\n",
    "    start_time = time.time() if measure_time else None\n",
    "\n",
    "    # Generate responses for the batch\n",
    "    outputs = model.generate(\n",
    "        prompts=None,\n",
    "        prompt_token_ids=inputs.input_ids,\n",
    "        sampling_params = vllm.SamplingParams(max_tokens=max_length, temperature=temperature),\n",
    "    )\n",
    "\n",
    "    # Measure time after generation\n",
    "    if measure_time:\n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "    else:\n",
    "        runtime = None\n",
    "\n",
    "    # Decode the batch of generated outputs\n",
    "    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "    # If the original input was a single prompt, return a single string, not a list\n",
    "    if len(prompts) == 1:\n",
    "        responses = responses[0]\n",
    "\n",
    "    # Return both the response and runtime if time measurement was requested\n",
    "    return (responses, runtime) if measure_time else responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Fk2goXdObF-"
   },
   "outputs": [],
   "source": [
    "# Batch inference\n",
    "many_responses, t = generate_response(model_vllm, many_prompts, measure_time=True)\n",
    "\n",
    "# Print results\n",
    "for res in many_responses:\n",
    "    print(\"---------------\")\n",
    "    print(res)\n",
    "print(\"\\n\\nRuntime after acceleration:\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sA2dFrQ4ObIx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjbJ_a_7ObNK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJ_VaGRNObQF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJqIke2l3fU3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Zs-iF825Dvq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d6eb16f5f5a4a5ca7d2f2caa21d4b5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13d2c8aca673481184654e1af26a2b73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2bb3efd6d1794a01b66c27b867409c2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93a79789eaf4404cbb1e66680275d347",
       "IPY_MODEL_f4a1e41696564260af9edf4c87f120ba",
       "IPY_MODEL_ab99f154499641c7b16a724d09409956"
      ],
      "layout": "IPY_MODEL_4f8eae8335f147bbab1107f619f30d24"
     }
    },
    "3f43a26e911f4f46b9802186942fe408": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f8eae8335f147bbab1107f619f30d24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "801549846c88431792724812adc8e408": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89c6f44f7f2c4adca32af7d6f3a58452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93a79789eaf4404cbb1e66680275d347": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f43a26e911f4f46b9802186942fe408",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_89c6f44f7f2c4adca32af7d6f3a58452",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "ab99f154499641c7b16a724d09409956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d6eb16f5f5a4a5ca7d2f2caa21d4b5d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_801549846c88431792724812adc8e408",
      "value": "â€‡2/2â€‡[00:47&lt;00:00,â€‡23.18s/it]"
     }
    },
    "f4a1e41696564260af9edf4c87f120ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5cb7c7eb6ef4069bd13099582cf5453",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_13d2c8aca673481184654e1af26a2b73",
      "value": 2
     }
    },
    "f5cb7c7eb6ef4069bd13099582cf5453": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
